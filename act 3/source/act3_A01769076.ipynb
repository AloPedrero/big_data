{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59188e2",
   "metadata": {},
   "source": [
    "# Actividad 3 | Aprendizaje supervisado y no supervisado\n",
    "---\n",
    "- Alonso Pedrero Mart√≠nez   |   A01769076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbe5b3",
   "metadata": {},
   "source": [
    "## Aprendizaje supervisado y no supervisado\n",
    "---\n",
    "\n",
    "- ‚Äã‚ÄãAprendizaje supervisado: es un enfoque de aprendizaje autom√°tico que se define por el uso de conjuntos de datos etiquetados. Estos conjuntos de datos est√°n dise√±ados para entrenar o supervisar algoritmos para clasificar datos o predecir resultados con precisi√≥n. Mediante entradas y salidas etiquetadas, el modelo puede medir su precisi√≥n y aprender con el tiempo.\n",
    "\n",
    "\n",
    "    - Clasificaci√≥n: los problemas utilizan un algoritmo para asignar con precisi√≥n los datos de prueba a categor√≠as espec√≠ficas, como separar manzanas de naranjas. O, en el mundo real, los algoritmos de aprendizaje supervisado pueden utilizarse para clasificar el correo no deseado en una carpeta separada de la bandeja de entrada. Los clasificadores lineales, las m√°quinas de vectores de soporte, los √°rboles de decisi√≥n y los bosques aleatorios son tipos comunes de algoritmos de clasificaci√≥n.\n",
    "\n",
    "\n",
    "    - Regresi√≥n: es otro tipo de m√©todo de aprendizaje supervisado que utiliza un algoritmo para comprender la relaci√≥n entre las variables dependientes e independientes. Los modelos de regresi√≥n son √∫tiles para predecir valores num√©ricos basados ‚Äã‚Äãen diferentes puntos de datos, como las proyecciones de ingresos por ventas para una empresa determinada. Algunos algoritmos de regresi√≥n populares son la regresi√≥n lineal, la regresi√≥n log√≠stica y la regresi√≥n polin√≥mica.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "\n",
    "- Aprendizaje no supervisado: utiliza algoritmos de aprendizaje autom√°tico para analizar y agrupar conjuntos de datos sin etiquetar. Estos algoritmos descubren patrones ocultos en los datos sin necesidad de intervenci√≥n humana.\n",
    "\n",
    "\n",
    "    - Agrupamiento: es una t√©cnica de miner√≠a de datos que permite agrupar datos sin etiquetar en funci√≥n de sus similitudes o diferencias. Por ejemplo, los algoritmos de agrupamiento K-medias asignan puntos de datos similares a grupos, donde el valor K representa el tama√±o de la agrupaci√≥n y la granularidad. Esta t√©cnica es √∫til para la segmentaci√≥n de mercado, la compresi√≥n de im√°genes, etc.\n",
    "\n",
    "\n",
    "    - Asociaci√≥n: es otro tipo de m√©todo de aprendizaje no supervisado que utiliza diferentes reglas para encontrar relaciones entre las variables de un conjunto de datos determinado. Estos m√©todos se utilizan con frecuencia para el an√°lisis de la cesta de la compra y los motores de recomendaci√≥n, como las recomendaciones de \"Los clientes que compraron este art√≠culo tambi√©n compraron\".\n",
    "\n",
    "\n",
    "    - Reducci√≥n de la dimensionalidad: es una t√©cnica de aprendizaje que se utiliza cuando el n√∫mero de caracter√≠sticas (o dimensiones) en un conjunto de datos determinado es demasiado elevado. Reduce la cantidad de datos de entrada a un tama√±o manejable, preservando al mismo tiempo su integridad. Esta t√©cnica se utiliza a menudo en la etapa de preprocesamiento de datos, como cuando los autocodificadores eliminan el ruido de los datos visuales para mejorar la calidad de la imagen.\n",
    "\n",
    "--- \n",
    "Referencia bibliogr√°fica:\n",
    "\n",
    "    Delua, J. (2025, April 17). Supervised vs unsupervised learning. Supervised versus unsupervised learning: What‚Äôs the difference? https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ed20b",
   "metadata": {},
   "source": [
    "## Aprendizaje supervisado y no supervisado en pySpark\n",
    "---\n",
    "üîπ Modelos de Aprendizaje Supervisado en PySpark\n",
    "\n",
    "üî∏ Clasificaci√≥n\n",
    "\n",
    "- LogisticRegression ‚Äì Regresi√≥n log√≠stica binaria o multinomial\n",
    "- DecisionTreeClassifier ‚Äì √Årbol de decisi√≥n para clasificaci√≥n\n",
    "- RandomForestClassifier ‚Äì Bosques aleatorios\n",
    "- GBTClassifier ‚Äì Gradient-Boosted Trees\n",
    "- NaiveBayes ‚Äì Clasificador basado en teorema de Bayes\n",
    "- MultilayerPerceptronClassifier ‚Äì Red neuronal multicapa\n",
    "- LinearSVC ‚Äì M√°quina de vectores de soporte lineal\n",
    "\n",
    "\n",
    "üî∏ Regresi√≥n\n",
    "\n",
    "- LinearRegression ‚Äì Regresi√≥n lineal\n",
    "- DecisionTreeRegressor ‚Äì √Årbol de decisi√≥n para regresi√≥n\n",
    "- RandomForestRegressor ‚Äì Bosques aleatorios\n",
    "- GBTRegressor ‚Äì Gradient-Boosted Trees\n",
    "- GeneralizedLinearRegression ‚Äì Regresi√≥n generalizada (Poisson, Binomial, etc.)\n",
    "- AFTSurvivalRegression ‚Äì An√°lisis de supervivencia\n",
    "\n",
    "\n",
    "üîπ Modelos de Aprendizaje No Supervisado en PySpark\n",
    "\n",
    "üî∏ Clustering\n",
    "\n",
    "- KMeans ‚Äì Clustering por K-medias\n",
    "- GaussianMixture ‚Äì Modelos de mezcla gaussiana (soft clustering)\n",
    "- BisectingKMeans ‚Äì Variante jer√°rquica de K-means\n",
    "- LDA ‚Äì An√°lisis de Dirichlet latente (para temas en texto)\n",
    "\n",
    "üî∏ Reducci√≥n de Dimensionalidad\n",
    "\n",
    "- PCA ‚Äì An√°lisis de Componentes Principales\n",
    "- SVD ‚Äì Descomposici√≥n en valores singulares (indirectamente a trav√©s de ALS y PCA)\n",
    "\n",
    "üîπ Otros Algoritmos √ötiles\n",
    "- ALS ‚Äì Alternating Least Squares (recomendadores colaborativos)\n",
    "- IsotonicRegression ‚Äì Regresi√≥n isot√≥nica (no lineal y no param√©trica)\n",
    "- OneVsRest ‚Äì Envoltorio para clasificaci√≥n multiclase\n",
    "- Pipeline ‚Äì Para encadenar transformaciones y modelos\n",
    "- CrossValidator y TrainValidationSplit ‚Äì Validaci√≥n cruzada y tuning de hiperpar√°metros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d27a9",
   "metadata": {},
   "source": [
    "## Selecci√≥n de los datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c68fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from pyspark.sql.types import StringType, DoubleType, FloatType\n",
    "import findspark\n",
    "\n",
    "from itertools import product\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56bf7ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alonsopedreromartinez/Documents/GitHub/big_data/act 3/act_3_big_data/lib/python3.12/site-packages/pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d76b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/21 19:30:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10c4ddbb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49847)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/alonsopedreromartinez/Documents/GitHub/big_data/act 3/act_3_big_data/lib/python3.12/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/alonsopedreromartinez/Documents/GitHub/big_data/act 3/act_3_big_data/lib/python3.12/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/Users/alonsopedreromartinez/Documents/GitHub/big_data/act 3/act_3_big_data/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alonsopedreromartinez/Documents/GitHub/big_data/act 3/act_3_big_data/lib/python3.12/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4910d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../files\"\n",
    "FILE = \"amazon_electronics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d85f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    @staticmethod\n",
    "    def open_csv_file(input_path : str, file_name : str):\n",
    "        \"\"\"\n",
    "        This method opens a csv file with pyspark\n",
    "        \"\"\"\n",
    "        csv_df = spark.read.csv(\n",
    "            path.join(input_path, file_name),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            multiLine=True,\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\"\n",
    "        )\n",
    "\n",
    "        csv_df.show(truncate=20)\n",
    "\n",
    "        return csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac930c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/21 19:30:27 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   22873041|R3ARRMDEGED8RD|B00KJWQIIC|     335625766|Plemo 14-Inch Lap...|              PC|          5|            0|          0|   N|                Y|Pleasantly surprised|I was very surpri...| 2015-08-31|        1|\n",
      "|         US|   30088427| RQ28TSA020Y6J|B013ALA9LA|     671157305|TP-Link OnHub AC1...|              PC|          5|           24|         31|   N|                N|OnHub is a pretty...|I am a Google emp...| 2015-08-31|        1|\n",
      "|         US|   20329786| RUXJRZCT6953M|B00PML2GQ8|     982036237|AmazonBasics USB ...|              PC|          1|            2|          2|   N|                N|None of them work...|Bought cables in ...| 2015-08-31|        0|\n",
      "|         US|   14215710| R7EO0UO6BPB71|B001NS0OZ4|     576587596|Transcend P8 15-i...|              PC|          1|            0|          0|   N|                Y|just keep searching.|nope, cheap and slow| 2015-08-31|        0|\n",
      "|         US|   38264512|R39NJY2YJ1JFSV|B00AQMTND2|     964759214|Aleratec SATA Dat...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Excellent! Great ...| 2015-08-31|        1|\n",
      "|         US|   30548466|R31SR7REWNX7CF|B00KX4TORI|     170101802|Kingston Digital ...|              PC|          5|            0|          0|   N|                Y|Good quality, wor...|Good quality,work...| 2015-08-31|        1|\n",
      "|         US|     589298| RVBP8I1R0CTZ8|B00P17WEMY|     206124740|White 9 Inch Unlo...|              PC|          3|            1|          2|   N|                Y|in fact this is t...|This demn tablet ...| 2015-08-31|        0|\n",
      "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|        1|\n",
      "|         US|   50728290|R23AICGEDAJQL1|B0098Y77OG|     177098042|                Acer|              PC|          1|            0|          0|   N|                Y|You get what you ...|After exactly 45 ...| 2015-08-31|        0|\n",
      "|         US|   37802374|R2EY3N4K9W19UP|B00IFYEYXC|     602496520|AzureWave Broadco...|              PC|          5|            3|          4|   N|                Y|Great for Windows...|Replaced my Intel...| 2015-08-31|        1|\n",
      "|         US|   52027882| RC9AW4HKJ016M|B0091ITP0S|     977217357|HDE Rotating iPad...|              PC|          1|            0|          0|   N|                Y|            One Star|IT HAS ALREADY CR...| 2015-08-31|        0|\n",
      "|         US|   41770239|R2ALWJE9N6ZBXD|B008I21EA2|     295632907|Linksys AC1750 Wi...|              PC|          1|            0|          0|   N|                N|   Very Disappointed|Very disappointed...| 2015-08-31|        0|\n",
      "|         US|   42560427|R2G5FPA4OX37GV|B00MRB7SBO|     922591915|iPad Pro 9.7, iPa...|              PC|          5|            1|          1|   N|                Y|          Five Stars|Works well. I use...| 2015-08-31|        1|\n",
      "|         US|   46345923|R1IKTSEVXSIMOD|B00LLER2CS|     997551273|SanDisk 16GB CZ43...|              PC|          5|            0|          0|   N|                Y|The encryption so...|The encryption so...| 2015-08-31|        1|\n",
      "|         US|   41751192|R2YA6G6SRFEWF6|B00B0CQCCC|     937999925|TRENDnet Wireless...|              PC|          1|            0|          1|   N|                Y|Didn't last 2 years.|I have owned this...| 2015-08-31|        0|\n",
      "|         US|   21176481| RS9H1N9I3Z1IA|B00GU8W5AE|      13865167|Redragon M901 PER...|              PC|          5|            0|          0|   N|                Y|Awesome gaming mouse|My first gaming m...| 2015-08-31|        1|\n",
      "|         US|   10674058| RKKLBI76VTDNT|B00XHMXJQ0|     967483469|Mudder MHL Adapte...|              PC|          1|            0|          0|   N|                Y|            One Star|I cannot get it t...| 2015-08-31|        0|\n",
      "|         US|   43341796|R2NJ3WFUS4E5G6|B00YGJJQ6U|     986548413|Fintie iPad Air 2...|              PC|          4|            0|          0|   N|                Y|Great choices of ...|Love that Finite ...| 2015-08-31|        1|\n",
      "|         US|   13232866|R21PTQNLGCBN0I|B00XMN20Y6|     873354048|Fintie iPad 2/3/4...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Nice color, I lov...| 2015-08-31|        1|\n",
      "|         US|   29333557|R3G4RT3EQ9RSY7|B00MA40W9I|     535866197|Egoway¬Æ New Lapto...|              PC|          1|            0|          0|   N|                Y|Totally wasted $6...|Totally wasted $6...| 2015-08-31|        0|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews = FileManager.open_csv_file(PATH, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e39c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Previously defined relevant columns for the activity.\n",
    "RELEVANT_COLUMNS_FOR_CHARACTERIZATION = [\n",
    "  \"star_rating\",\n",
    "  \"helpful_votes\",\n",
    "  \"total_votes\",\n",
    "  \"vine\",\n",
    "  \"verified_purchase\",\n",
    "  \"review_date\",\n",
    "  \"sentiment\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7e3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_filtered = df_reviews.select(*RELEVANT_COLUMNS_FOR_CHARACTERIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507d63d",
   "metadata": {},
   "source": [
    "### Generaci√≥n de las particiones\n",
    "\n",
    "Se implement√≥ un procedimiento autom√°tico en PySpark que:\n",
    "\n",
    "1. Filtra los registros de la base de datos que cumplen con cada combinaci√≥n de valores.\n",
    "2. Almacena cada subconjunto en un diccionario indexado por nombre de combinaci√≥n (ej. \"R5_VPY_VN\" para `star_rating`=5, `verified_purchase`=Y, `vine`=N).\n",
    "3. Imprime la cantidad de registros por partici√≥n para control y trazabilidad.\n",
    "\n",
    "Las particiones con muy pocos registros pueden ser descartadas en etapas posteriores para evitar problemas en el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad608743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitioningManager:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_probabilities(df, cols):\n",
    "        \"\"\"\n",
    "        Computes and returns the probability of each combination of values in the specified columns.\n",
    "        \"\"\"\n",
    "        total_count = df.count()\n",
    "        return df.groupBy(cols).count() \\\n",
    "                 .withColumn(\"probability\", F.round(F.col(\"count\") / total_count, 6)) \\\n",
    "                 .orderBy(\"probability\", ascending=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_partition(df, star_rating, verified_purchase, vine):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame by specific values for rating, verified purchase, and vine.\n",
    "        \"\"\"\n",
    "        return df.filter(\n",
    "            (F.col(\"star_rating\") == star_rating) &\n",
    "            (F.col(\"verified_purchase\") == verified_purchase) &\n",
    "            (F.col(\"vine\") == vine)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_all_partitions(df, min_probability=0.0001):\n",
    "        \"\"\"\n",
    "        Generates partitions only for combinations whose joint probability is above min_probability.\n",
    "        \"\"\"\n",
    "\n",
    "        prob_df = PartitioningManager.compute_probabilities(\n",
    "            df, [\"star_rating\", \"verified_purchase\", \"vine\"]\n",
    "        )\n",
    "\n",
    "        filtered_combinations = prob_df.filter(\n",
    "            F.col(\"probability\") >= min_probability\n",
    "        ).select(\"star_rating\", \"verified_purchase\", \"vine\").collect()\n",
    "\n",
    "        partitions = {}\n",
    "        for row in filtered_combinations:\n",
    "            rating = row[\"star_rating\"]\n",
    "            purchase = row[\"verified_purchase\"]\n",
    "            vine = row[\"vine\"]\n",
    "\n",
    "            key = f\"R{rating}_VP{purchase}_V{vine}\"\n",
    "            filtered = PartitioningManager.filter_partition(df, rating, purchase, vine)\n",
    "            partitions[key] = filtered\n",
    "            print(f\"Partition {key} created with {filtered.count()} records.\")\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def stratified_sample_partitioned_data(partitions_dict, label_col=\"sentiment\", fraction=0.3, min_rows=50):\n",
    "        \"\"\"\n",
    "        Applies stratified sampling to each partition based on sentiment.\n",
    "        \"\"\"\n",
    "        sampled_partitions = {}\n",
    "\n",
    "        for key, df in partitions_dict.items():\n",
    "            count = df.count()\n",
    "\n",
    "            if count < min_rows:\n",
    "                print(f\"Skipping partition {key} ‚Äî only {count} rows (<{min_rows})\")\n",
    "                continue\n",
    "\n",
    "            sentiments = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            fractions = {s: fraction for s in sentiments}\n",
    "\n",
    "            sampled_df = df.sampleBy(label_col, fractions, seed=42)\n",
    "            sampled_partitions[key] = sampled_df\n",
    "            print(f\"Sampled {sampled_df.count()} rows from partition {key} (original: {count})\")\n",
    "\n",
    "        return sampled_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dbca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN created with 3679909 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPY_VN created with 1019728 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPY_VN created with 603371 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPY_VN created with 443364 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VN created with 410073 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPY_VN created with 300544 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VN created with 152779 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VN created with 135197 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VN created with 65398 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VN created with 59973 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VY created with 15604 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VY created with 13240 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VY created with 4886 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VY created with 1634 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VY created with 705 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VY created with 101 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "partitions = PartitioningManager.generate_all_partitions(df_reviews, min_probability=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec01c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   22873041|R3ARRMDEGED8RD|B00KJWQIIC|     335625766|Plemo 14-Inch Lap...|              PC|          5|            0|          0|   N|                Y|Pleasantly surprised|I was very surpri...| 2015-08-31|        1|\n",
      "|         US|   38264512|R39NJY2YJ1JFSV|B00AQMTND2|     964759214|Aleratec SATA Dat...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Excellent! Great ...| 2015-08-31|        1|\n",
      "|         US|   30548466|R31SR7REWNX7CF|B00KX4TORI|     170101802|Kingston Digital ...|              PC|          5|            0|          0|   N|                Y|Good quality, wor...|Good quality,work...| 2015-08-31|        1|\n",
      "|         US|   37802374|R2EY3N4K9W19UP|B00IFYEYXC|     602496520|AzureWave Broadco...|              PC|          5|            3|          4|   N|                Y|Great for Windows...|Replaced my Intel...| 2015-08-31|        1|\n",
      "|         US|   42560427|R2G5FPA4OX37GV|B00MRB7SBO|     922591915|iPad Pro 9.7, iPa...|              PC|          5|            1|          1|   N|                Y|          Five Stars|Works well. I use...| 2015-08-31|        1|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#¬†Sample of one of the generated partitions.\n",
    "partitions[\"R5_VPY_VN\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ba004",
   "metadata": {},
   "source": [
    "## 3.T√©cnica de muestreo aplicada por partici√≥n\n",
    "\n",
    "Una vez construidas las particiones, se aplic√≥ una t√©cnica de **muestreo estratificado** sobre cada subconjunto, usando la variable `sentiment` como variable de estratificaci√≥n. Esto asegura que cada muestra mantenga la proporci√≥n original de clases de sentimiento en la partici√≥n.\n",
    "\n",
    "Para evitar particiones con tama√±os insuficientes, se defini√≥ un umbral m√≠nimo (`min_rows`) que descarta autom√°ticamente las particiones con muy pocos registros.\n",
    "\n",
    "Adem√°s, se permiti√≥ configurar el porcentaje de muestreo (`fraction`) por clase de sentimiento. Esto ofrece flexibilidad para ajustar el tama√±o del conjunto de entrenamiento o validaci√≥n seg√∫n necesidades posteriores.\n",
    "\n",
    "#### Justificaci√≥n del muestreo estratificado\n",
    "\n",
    "* **Preservaci√≥n del equilibrio de clases**: Al muestrear por clase de sentimiento se evitan sesgos por clases desbalanceadas.\n",
    "* **Relevancia contextual**: Al aplicar el muestreo dentro de cada partici√≥n (y no sobre la base completa), se conserva la variabilidad contextual de las rese√±as.\n",
    "* **Evita el submuestreo accidental**: Las particiones muy peque√±as son descartadas de forma controlada, garantizando que el conjunto final tenga representatividad suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb6bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 184082 rows from partition R5_VPY_VN (original: 3679909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 51148 rows from partition R4_VPY_VN (original: 1019728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 30183 rows from partition R1_VPY_VN (original: 603371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 22223 rows from partition R3_VPY_VN (original: 443364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 20507 rows from partition R5_VPN_VN (original: 410073)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 15034 rows from partition R2_VPY_VN (original: 300544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 7595 rows from partition R1_VPN_VN (original: 152779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 6708 rows from partition R4_VPN_VN (original: 135197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3345 rows from partition R3_VPN_VN (original: 65398)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3066 rows from partition R2_VPN_VN (original: 59973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 817 rows from partition R5_VPN_VY (original: 15604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 724 rows from partition R4_VPN_VY (original: 13240)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 266 rows from partition R3_VPN_VY (original: 4886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 93 rows from partition R2_VPN_VY (original: 1634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 35 rows from partition R1_VPN_VY (original: 705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3 rows from partition R5_VPY_VY (original: 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_partitions = PartitioningManager.stratified_sample_partitioned_data(partitions, fraction=0.05, min_rows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792c0d5",
   "metadata": {},
   "source": [
    "## Preparaci√≥n de los datos\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897e09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalAnalysisHelper():\n",
    "    @staticmethod\n",
    "    def dataset_dimensions(df_input):\n",
    "        print(\"columns in the dataset:\", len(df_input.columns))\n",
    "        print(\"rows in the dataset:\", df_input.count())\n",
    "\n",
    "    @staticmethod\n",
    "    def schema_information(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the current schema of the data.\n",
    "        \"\"\"\n",
    "        df_input.printSchema()\n",
    "\n",
    "    @staticmethod\n",
    "    def descriptive_statistics(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the descriptive statistics of the data.\n",
    "        \"\"\"\n",
    "        df_input.summary().show(truncate=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def missing_values_table(df_input):\n",
    "        \"\"\"\n",
    "        Displays a table with the count of missing values per column.\n",
    "        \"\"\"\n",
    "        missing_exprs = []\n",
    "        \n",
    "        for c in df_input.schema.fields:\n",
    "            field_name = c.name\n",
    "            field_type = c.dataType\n",
    "            \n",
    "            if isinstance(field_type, (DoubleType, FloatType)):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | isnan(col(field_name)), field_name)).alias(field_name)\n",
    "                )\n",
    "            elif isinstance(field_type, StringType):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | (col(field_name) == \"\"), field_name)).alias(field_name)\n",
    "                )\n",
    "            else:\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull(), field_name)).alias(field_name)\n",
    "                )\n",
    "\n",
    "        df_missing_values = df_input.select(missing_exprs)\n",
    "\n",
    "        return df_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "377c254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualAnalysisHelper:\n",
    "    @staticmethod\n",
    "    def missing_values_plot(df_missing_values):\n",
    "        \"\"\"\n",
    "        This method shows the missing values and plots them.\n",
    "        \"\"\"\n",
    "        df_missing_values_pd = df_missing_values.toPandas()\n",
    "\n",
    "        df_missing_values_pd = df_missing_values_pd.melt(var_name=\"column\", value_name=\"missing_count\")\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=\"column\", y=\"missing_count\", data=df_missing_values_pd)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title(\"Missing Values per Column\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def column_distribution_plot(df_input, column_name, x_label, y_label):\n",
    "        \"\"\"\n",
    "        This method plots the distribution of the 'star_rating' column.\n",
    "        \"\"\"\n",
    "\n",
    "        rating_counts = (\n",
    "            df_input.groupBy(column_name)\n",
    "            .count()\n",
    "            .orderBy(column_name)\n",
    "        )\n",
    "\n",
    "        plot_data = rating_counts.toPandas()\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x=column_name, y=\"count\", data=plot_data)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(\"Distribution of Values\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b228315",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mStatisticalAnalysisHelper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_partitions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mStatisticalAnalysisHelper.dataset_dimensions\u001b[39m\u001b[34m(df_input)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdataset_dimensions\u001b[39m(df_input):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcolumns in the dataset:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdf_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m))\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mrows in the dataset:\u001b[39m\u001b[33m\"\u001b[39m, df_input.count())\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.dataset_dimensions(sampled_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticalAnalysisHelper.schema_information(sampled_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticalAnalysisHelper.descriptive_statistics(sampled_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = StatisticalAnalysisHelper.missing_values_table(sampled_partitions)\n",
    "missing_values.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc581ec",
   "metadata": {},
   "source": [
    "It's worth noting that this dataset seems somewhat suspicious. It doesn't have any missing values, which is great for our use case as it simplifies the analysis by eliminating the need to handle incomplete data.\n",
    "\n",
    "However, the absence of missing values is unusual in real-world datasets, where incomplete records are often present. This raises the possibility that the dataset might not be entirely authentic or could have been autogenerated. While it's impossible to confirm, the dataset‚Äôs structure and the perfect data cleanliness suggest that it may not come from a natural source. This could potentially affect the reliability of any analysis performed using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualAnalysisHelper.missing_values_plot(sampled_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8ce89",
   "metadata": {},
   "source": [
    "## Preparaci√≥n del conjunto de entrenamiento y prueba\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a4cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_3_big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
