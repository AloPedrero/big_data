{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59188e2",
   "metadata": {},
   "source": [
    "# Actividad 4 | M√©tricas de calidad de resultados\n",
    "---\n",
    "- Alonso Pedrero Mart√≠nez   |   A01769076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d27a9",
   "metadata": {},
   "source": [
    "## Selecci√≥n de los datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47c68fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from pyspark.sql.types import StringType, DoubleType, FloatType\n",
    "import findspark\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import pandas as pd\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56bf7ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alonsopedreromartinez/Documents/GitHub/big_data/act 4/act_4_big_data/lib/python3.12/site-packages/pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d76b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/08 22:20:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x122b38710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4910d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../files\"\n",
    "FILE = \"amazon_electronics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d85f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    @staticmethod\n",
    "    def open_csv_file(input_path : str, file_name : str):\n",
    "        \"\"\"\n",
    "        This method opens a csv file with pyspark\n",
    "        \"\"\"\n",
    "        csv_df = spark.read.csv(\n",
    "            path.join(input_path, file_name),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            multiLine=True,\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\"\n",
    "        )\n",
    "\n",
    "        csv_df.show(truncate=20)\n",
    "\n",
    "        return csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac930c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   22873041|R3ARRMDEGED8RD|B00KJWQIIC|     335625766|Plemo 14-Inch Lap...|              PC|          5|            0|          0|   N|                Y|Pleasantly surprised|I was very surpri...| 2015-08-31|        1|\n",
      "|         US|   30088427| RQ28TSA020Y6J|B013ALA9LA|     671157305|TP-Link OnHub AC1...|              PC|          5|           24|         31|   N|                N|OnHub is a pretty...|I am a Google emp...| 2015-08-31|        1|\n",
      "|         US|   20329786| RUXJRZCT6953M|B00PML2GQ8|     982036237|AmazonBasics USB ...|              PC|          1|            2|          2|   N|                N|None of them work...|Bought cables in ...| 2015-08-31|        0|\n",
      "|         US|   14215710| R7EO0UO6BPB71|B001NS0OZ4|     576587596|Transcend P8 15-i...|              PC|          1|            0|          0|   N|                Y|just keep searching.|nope, cheap and slow| 2015-08-31|        0|\n",
      "|         US|   38264512|R39NJY2YJ1JFSV|B00AQMTND2|     964759214|Aleratec SATA Dat...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Excellent! Great ...| 2015-08-31|        1|\n",
      "|         US|   30548466|R31SR7REWNX7CF|B00KX4TORI|     170101802|Kingston Digital ...|              PC|          5|            0|          0|   N|                Y|Good quality, wor...|Good quality,work...| 2015-08-31|        1|\n",
      "|         US|     589298| RVBP8I1R0CTZ8|B00P17WEMY|     206124740|White 9 Inch Unlo...|              PC|          3|            1|          2|   N|                Y|in fact this is t...|This demn tablet ...| 2015-08-31|        0|\n",
      "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|        1|\n",
      "|         US|   50728290|R23AICGEDAJQL1|B0098Y77OG|     177098042|                Acer|              PC|          1|            0|          0|   N|                Y|You get what you ...|After exactly 45 ...| 2015-08-31|        0|\n",
      "|         US|   37802374|R2EY3N4K9W19UP|B00IFYEYXC|     602496520|AzureWave Broadco...|              PC|          5|            3|          4|   N|                Y|Great for Windows...|Replaced my Intel...| 2015-08-31|        1|\n",
      "|         US|   52027882| RC9AW4HKJ016M|B0091ITP0S|     977217357|HDE Rotating iPad...|              PC|          1|            0|          0|   N|                Y|            One Star|IT HAS ALREADY CR...| 2015-08-31|        0|\n",
      "|         US|   41770239|R2ALWJE9N6ZBXD|B008I21EA2|     295632907|Linksys AC1750 Wi...|              PC|          1|            0|          0|   N|                N|   Very Disappointed|Very disappointed...| 2015-08-31|        0|\n",
      "|         US|   42560427|R2G5FPA4OX37GV|B00MRB7SBO|     922591915|iPad Pro 9.7, iPa...|              PC|          5|            1|          1|   N|                Y|          Five Stars|Works well. I use...| 2015-08-31|        1|\n",
      "|         US|   46345923|R1IKTSEVXSIMOD|B00LLER2CS|     997551273|SanDisk 16GB CZ43...|              PC|          5|            0|          0|   N|                Y|The encryption so...|The encryption so...| 2015-08-31|        1|\n",
      "|         US|   41751192|R2YA6G6SRFEWF6|B00B0CQCCC|     937999925|TRENDnet Wireless...|              PC|          1|            0|          1|   N|                Y|Didn't last 2 years.|I have owned this...| 2015-08-31|        0|\n",
      "|         US|   21176481| RS9H1N9I3Z1IA|B00GU8W5AE|      13865167|Redragon M901 PER...|              PC|          5|            0|          0|   N|                Y|Awesome gaming mouse|My first gaming m...| 2015-08-31|        1|\n",
      "|         US|   10674058| RKKLBI76VTDNT|B00XHMXJQ0|     967483469|Mudder MHL Adapte...|              PC|          1|            0|          0|   N|                Y|            One Star|I cannot get it t...| 2015-08-31|        0|\n",
      "|         US|   43341796|R2NJ3WFUS4E5G6|B00YGJJQ6U|     986548413|Fintie iPad Air 2...|              PC|          4|            0|          0|   N|                Y|Great choices of ...|Love that Finite ...| 2015-08-31|        1|\n",
      "|         US|   13232866|R21PTQNLGCBN0I|B00XMN20Y6|     873354048|Fintie iPad 2/3/4...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Nice color, I lov...| 2015-08-31|        1|\n",
      "|         US|   29333557|R3G4RT3EQ9RSY7|B00MA40W9I|     535866197|Egoway¬Æ New Lapto...|              PC|          1|            0|          0|   N|                Y|Totally wasted $6...|Totally wasted $6...| 2015-08-31|        0|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_reviews = FileManager.open_csv_file(PATH, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e39c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Previously defined relevant columns for the activity.\n",
    "RELEVANT_COLUMNS_FOR_CHARACTERIZATION = [\n",
    "  \"star_rating\",\n",
    "  \"helpful_votes\",\n",
    "  \"total_votes\",\n",
    "  \"vine\",\n",
    "  \"verified_purchase\",\n",
    "  \"review_date\",\n",
    "  \"sentiment\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7e3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_filtered = df_reviews.select(*RELEVANT_COLUMNS_FOR_CHARACTERIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507d63d",
   "metadata": {},
   "source": [
    "### Generaci√≥n de las particiones\n",
    "\n",
    "Se implement√≥ un procedimiento autom√°tico en PySpark que:\n",
    "\n",
    "1. Filtra los registros de la base de datos que cumplen con cada combinaci√≥n de valores.\n",
    "2. Almacena cada subconjunto en un diccionario indexado por nombre de combinaci√≥n (ej. \"R5_VPY_VN\" para `star_rating`=5, `verified_purchase`=Y, `vine`=N).\n",
    "3. Imprime la cantidad de registros por partici√≥n para control y trazabilidad.\n",
    "\n",
    "Las particiones con muy pocos registros pueden ser descartadas en etapas posteriores para evitar problemas en el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad608743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitioningManager:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_probabilities(df, cols):\n",
    "        \"\"\"\n",
    "        Computes and returns the probability of each combination of values in the specified columns.\n",
    "        \"\"\"\n",
    "        total_count = df.count()\n",
    "        return df.groupBy(cols).count() \\\n",
    "                 .withColumn(\"probability\", F.round(F.col(\"count\") / total_count, 6)) \\\n",
    "                 .orderBy(\"probability\", ascending=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_partition(df, star_rating, verified_purchase, vine):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame by specific values for rating, verified purchase, and vine.\n",
    "        \"\"\"\n",
    "        return df.filter(\n",
    "            (F.col(\"star_rating\") == star_rating) &\n",
    "            (F.col(\"verified_purchase\") == verified_purchase) &\n",
    "            (F.col(\"vine\") == vine)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_all_partitions(df, min_probability=0.0001):\n",
    "        \"\"\"\n",
    "        Generates partitions only for combinations whose joint probability is above min_probability.\n",
    "        \"\"\"\n",
    "\n",
    "        prob_df = PartitioningManager.compute_probabilities(\n",
    "            df, [\"star_rating\", \"verified_purchase\", \"vine\"]\n",
    "        )\n",
    "\n",
    "        filtered_combinations = prob_df.filter(\n",
    "            F.col(\"probability\") >= min_probability\n",
    "        ).select(\"star_rating\", \"verified_purchase\", \"vine\").collect()\n",
    "\n",
    "        partitions = {}\n",
    "        for row in filtered_combinations:\n",
    "            rating = row[\"star_rating\"]\n",
    "            purchase = row[\"verified_purchase\"]\n",
    "            vine = row[\"vine\"]\n",
    "\n",
    "            key = f\"R{rating}_VP{purchase}_V{vine}\"\n",
    "            filtered = PartitioningManager.filter_partition(df, rating, purchase, vine)\n",
    "            partitions[key] = filtered\n",
    "            print(f\"Partition {key} created with {filtered.count()} records.\")\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def stratified_sample_partitioned_data(partitions_dict, label_col=\"sentiment\", fraction=0.3, min_rows=50):\n",
    "        \"\"\"\n",
    "        Applies stratified sampling to each partition based on sentiment.\n",
    "        \"\"\"\n",
    "        sampled_partitions = {}\n",
    "\n",
    "        for key, df in partitions_dict.items():\n",
    "            count = df.count()\n",
    "\n",
    "            if count < min_rows:\n",
    "                print(f\"Skipping partition {key} ‚Äî only {count} rows (<{min_rows})\")\n",
    "                continue\n",
    "\n",
    "            sentiments = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            fractions = {s: fraction for s in sentiments}\n",
    "\n",
    "            sampled_df = df.sampleBy(label_col, fractions, seed=42)\n",
    "            sampled_partitions[key] = sampled_df\n",
    "            print(f\"Sampled {sampled_df.count()} rows from partition {key} (original: {count})\")\n",
    "\n",
    "        return sampled_partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def build_combined_sample(partitions_sampled_dict):\n",
    "        \"\"\"\n",
    "        Unites all sampled partitions into a single DataFrame (M).\n",
    "        This helps reduce computational load while maintaining diversity.\n",
    "        \"\"\"\n",
    "        if not partitions_sampled_dict:\n",
    "            raise ValueError(\"No partitions provided for sample combination.\")\n",
    "\n",
    "        combined_df = None\n",
    "        for key, df in partitions_sampled_dict.items():\n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = combined_df.union(df)\n",
    "            print(f\"Partition {key} added to the combined sample.\")\n",
    "\n",
    "        print(f\"Total records in combined sample: {combined_df.count()}\")\n",
    "        return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dbca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN created with 3679909 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPY_VN created with 1019728 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPY_VN created with 603371 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPY_VN created with 443364 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VN created with 410073 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPY_VN created with 300544 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VN created with 152779 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VN created with 135197 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VN created with 65398 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VN created with 59973 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VY created with 15604 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VY created with 13240 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VY created with 4886 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VY created with 1634 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VY created with 705 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VY created with 101 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "partitions = PartitioningManager.generate_all_partitions(df_reviews, min_probability=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec01c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|        1|\n",
      "|         US|   43341796|R2NJ3WFUS4E5G6|B00YGJJQ6U|     986548413|Fintie iPad Air 2...|              PC|          4|            0|          0|   N|                Y|Great choices of ...|Love that Finite ...| 2015-08-31|        1|\n",
      "|         US|   12246861|R2WV71J88ERO3H|B00J082KFQ|      55148922|Case Logic BPCA-1...|              PC|          4|            0|          0|   N|                Y|  Slimmer laptop bag|This is pretty mu...| 2015-08-31|        1|\n",
      "|         US|   23531656|R1DG8L9CSXXPJK|B008IFXQFU|     422480840|TP- Link Wireless...|              PC|          4|            0|          0|   N|                Y|          Four Stars|It works for its ...| 2015-08-31|        1|\n",
      "|         US|   28827675|R1PHDY2F126NRD|B00MB80WE8|     763447327|Prontotec Axius N...|              PC|          4|            0|          0|   N|                Y|So affordable & w...|This was bought f...| 2015-08-31|        1|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#¬†Sample of one of the generated partitions.\n",
    "partitions[\"R4_VPY_VN\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ba004",
   "metadata": {},
   "source": [
    "## T√©cnica de muestreo aplicada por partici√≥n\n",
    "\n",
    "Una vez construidas las particiones, se aplic√≥ una t√©cnica de **muestreo estratificado** sobre cada subconjunto, usando la variable `sentiment` como variable de estratificaci√≥n. Esto asegura que cada muestra mantenga la proporci√≥n original de clases de sentimiento en la partici√≥n.\n",
    "\n",
    "Para evitar particiones con tama√±os insuficientes, se defini√≥ un umbral m√≠nimo (`min_rows`) que descarta autom√°ticamente las particiones con muy pocos registros.\n",
    "\n",
    "Adem√°s, se permiti√≥ configurar el porcentaje de muestreo (`fraction`) por clase de sentimiento. Esto ofrece flexibilidad para ajustar el tama√±o del conjunto de entrenamiento o validaci√≥n seg√∫n necesidades posteriores.\n",
    "\n",
    "#### Justificaci√≥n del muestreo estratificado\n",
    "\n",
    "* **Preservaci√≥n del equilibrio de clases**: Al muestrear por clase de sentimiento se evitan sesgos por clases desbalanceadas.\n",
    "* **Relevancia contextual**: Al aplicar el muestreo dentro de cada partici√≥n (y no sobre la base completa), se conserva la variabilidad contextual de las rese√±as.\n",
    "* **Evita el submuestreo accidental**: Las particiones muy peque√±as son descartadas de forma controlada, garantizando que el conjunto final tenga representatividad suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb6bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 184082 rows from partition R5_VPY_VN (original: 3679909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 51148 rows from partition R4_VPY_VN (original: 1019728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 30183 rows from partition R1_VPY_VN (original: 603371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 22223 rows from partition R3_VPY_VN (original: 443364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 20507 rows from partition R5_VPN_VN (original: 410073)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 15034 rows from partition R2_VPY_VN (original: 300544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 7595 rows from partition R1_VPN_VN (original: 152779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 6708 rows from partition R4_VPN_VN (original: 135197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3345 rows from partition R3_VPN_VN (original: 65398)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3066 rows from partition R2_VPN_VN (original: 59973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 817 rows from partition R5_VPN_VY (original: 15604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 724 rows from partition R4_VPN_VY (original: 13240)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 266 rows from partition R3_VPN_VY (original: 4886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 93 rows from partition R2_VPN_VY (original: 1634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 35 rows from partition R1_VPN_VY (original: 705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3 rows from partition R5_VPY_VY (original: 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_partitions = PartitioningManager.stratified_sample_partitioned_data(partitions, fraction=0.05, min_rows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e764fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN added to the combined sample.\n",
      "Partition R4_VPY_VN added to the combined sample.\n",
      "Partition R1_VPY_VN added to the combined sample.\n",
      "Partition R3_VPY_VN added to the combined sample.\n",
      "Partition R5_VPN_VN added to the combined sample.\n",
      "Partition R2_VPY_VN added to the combined sample.\n",
      "Partition R1_VPN_VN added to the combined sample.\n",
      "Partition R4_VPN_VN added to the combined sample.\n",
      "Partition R3_VPN_VN added to the combined sample.\n",
      "Partition R2_VPN_VN added to the combined sample.\n",
      "Partition R5_VPN_VY added to the combined sample.\n",
      "Partition R4_VPN_VY added to the combined sample.\n",
      "Partition R3_VPN_VY added to the combined sample.\n",
      "Partition R2_VPN_VY added to the combined sample.\n",
      "Partition R1_VPN_VY added to the combined sample.\n",
      "Partition R5_VPY_VY added to the combined sample.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in combined sample: 345829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sample_M = PartitioningManager.build_combined_sample(sampled_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792c0d5",
   "metadata": {},
   "source": [
    "## Preparaci√≥n del conjunto de entrenamiento y prueba\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897e09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalAnalysisHelper():\n",
    "    @staticmethod\n",
    "    def dataset_dimensions(df_input):\n",
    "        print(\"columns in the dataset:\", len(df_input.columns))\n",
    "        print(\"rows in the dataset:\", df_input.count())\n",
    "\n",
    "    @staticmethod\n",
    "    def schema_information(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the current schema of the data.\n",
    "        \"\"\"\n",
    "        df_input.printSchema()\n",
    "\n",
    "    @staticmethod\n",
    "    def descriptive_statistics(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the descriptive statistics of the data.\n",
    "        \"\"\"\n",
    "        df_input.summary().show(truncate=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def missing_values_table(df_input):\n",
    "        \"\"\"\n",
    "        Displays a table with the count of missing values per column.\n",
    "        \"\"\"\n",
    "        missing_exprs = []\n",
    "        \n",
    "        for c in df_input.schema.fields:\n",
    "            field_name = c.name\n",
    "            field_type = c.dataType\n",
    "            \n",
    "            if isinstance(field_type, (DoubleType, FloatType)):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | isnan(col(field_name)), field_name)).alias(field_name)\n",
    "                )\n",
    "            elif isinstance(field_type, StringType):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | (col(field_name) == \"\"), field_name)).alias(field_name)\n",
    "                )\n",
    "            else:\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull(), field_name)).alias(field_name)\n",
    "                )\n",
    "\n",
    "        df_missing_values = df_input.select(missing_exprs)\n",
    "\n",
    "        return df_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b228315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in the dataset: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 210:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows in the dataset: 345829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.dataset_dimensions(df_sample_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8615d944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: integer (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.schema_information(df_sample_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055ddf0",
   "metadata": {},
   "source": [
    "Este esquema describe la estructura de un conjunto de datos de rese√±as de productos, ya preparado para an√°lisis en PySpark. Todas las columnas est√°n correctamente tipadas para facilitar tareas de preprocesamiento, modelado y visualizaci√≥n, por lo que no requieren cambios de tipo adicionales. A continuaci√≥n se describe cada variable:\n",
    "\n",
    "- marketplace (string): C√≥digo del pa√≠s o regi√≥n del mercado (por ejemplo, \"US\", \"MX\").\n",
    "- customer_id (integer): Identificador √∫nico del cliente que realiz√≥ la rese√±a.\n",
    "- review_id (string): Identificador √∫nico de la rese√±a.\n",
    "- product_id (string): Identificador √∫nico del producto rese√±ado.\n",
    "- product_parent (integer): ID grupal de productos relacionados (puede representar variantes).\n",
    "- product_title (string): Nombre o t√≠tulo del producto.\n",
    "- product_category (string): Categor√≠a del producto.\n",
    "- star_rating (integer): Calificaci√≥n en estrellas otorgada al producto (usualmente de 1 a 5).\n",
    "- helpful_votes (integer): N√∫mero de votos que consideraron √∫til la rese√±a.\n",
    "- total_votes (integer): N√∫mero total de votos recibidos por la rese√±a.\n",
    "- vine (string): Indica si la rese√±a es parte del programa Vine (\"Y\" o \"N\").\n",
    "- verified_purchase (string): Indica si la rese√±a proviene de una compra verificada (\"Y\" o \"N\").\n",
    "- review_headline (string): T√≠tulo de la rese√±a.\n",
    "- review_body (string): Cuerpo completo del texto de la rese√±a.\n",
    "- review_date (date): Fecha en que se public√≥ la rese√±a.\n",
    "- sentiment (integer): Etiqueta de sentimiento preprocesada, usada como variable objetivo para modelos supervisados (por ejemplo, 1 = positivo, 0 = negativo).\n",
    "\n",
    "\n",
    "Este esquema permite realizar tanto an√°lisis exploratorio como modelado predictivo con modelos de machine learning supervisados y no supervisados en PySpark sin necesidad de transformaci√≥n adicional de tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc7f9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 22:32:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 213:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|summary|marketplace|customer_id         |review_id     |product_id         |product_parent      |product_title                                                                                 |product_category|star_rating       |helpful_votes     |total_votes       |vine  |verified_purchase|review_headline                   |review_body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |sentiment          |\n",
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|count  |345829     |345829              |345829        |345829             |345829              |345829                                                                                        |345829          |345829            |345829            |345829            |345829|345829           |345829                            |345829                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |345829             |\n",
      "|mean   |NULL       |2.7991213281448346E7|NULL          |7.765654862257862E9|4.9180527983244896E8|1784.6153846153845                                                                            |NULL            |4.086025174291341 |1.4509280598214724|1.9227595140951164|NULL  |NULL             |696.6666666666666                 |3.75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0.763351251630141  |\n",
      "|stddev |NULL       |1.5591539570539333E7|NULL          |3.572692350202524E9|2.8897488575035936E8|1583.630463887913                                                                             |NULL            |1.3622617427344736|46.442456581791674|49.08379117291993 |NULL  |NULL             |1254.9528543601416                |4.272001872658765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |0.42502545879407655|\n",
      "|min    |US         |10157               |R1000CCCY44YL4|0132793040         |1602                |#1080CC Pelican HardBack Case (with Computer Case Liner)                                      |PC              |1                 |0                 |0                 |N     |N                |\u001a\u001a\u001a\u001a\u001a\u001a                            |\u001d\u001dWhile the texture and feel of this keyboard iPad case feels pretty good, I have to agree with another Vine reviewer that the keyboard is awkward. While the keyboard is about the size of a regular wireless Apple keyboard, the keys seem to be moved a little to the left, so it doesn't feel like normal typing. Even more problematic is when you click the space key, your thumb hits up against the edge of the case. So the basic poor design of the keyboard makes it not comfortable for typing, unfortunately.|0                  |\n",
      "|25%    |NULL       |14401893            |NULL          |6.465354276E9      |242889999           |700.0                                                                                         |NULL            |4                 |0                 |0                 |NULL  |NULL             |5.0                               |1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|50%    |NULL       |26724274            |NULL          |9.875971812E9      |488442804           |700.0                                                                                         |NULL            |5                 |0                 |0                 |NULL  |NULL             |7.0                               |1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|75%    |NULL       |42723957            |NULL          |9.966634975E9      |743709280           |3911.0                                                                                        |NULL            |5                 |1                 |1                 |NULL  |NULL             |1333.0                            |3.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|max    |US         |53096561            |RZZZD8HUINFU1 |B01MU3GE5L         |999982788           |‚òÜ 4 Port Powered USB 3.0 Hub Portable for MacBook Air, Windows 8 Tablet PC ‚òÜ Multiple Splitter|PC              |5                 |24714             |26143             |Y     |Y                |üòí Worst purchase I've made online|üòçüëçüëç                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |1                  |\n",
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.descriptive_statistics(df_sample_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c34cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "|marketplace|customer_id|review_id|product_id|product_parent|product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|review_headline|review_body|review_date|sentiment|\n",
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "|0          |0          |0        |0         |0             |0            |0               |0          |0            |0          |0   |0                |0              |0          |0          |0        |\n",
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_values = StatisticalAnalysisHelper.missing_values_table(df_sample_M)\n",
    "missing_values.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a51a9b",
   "metadata": {},
   "source": [
    "El contenido mostrado indica que el dataset no contiene valores nulos, vac√≠os ni faltantes en ninguna de sus columnas, lo cual es ideal para su uso en modelos de machine learning. Todas las columnas tienen registros v√°lidos, incluso aquellas de tipo texto, entero o fecha. La fila con valores \"0\" representa probablemente un ejemplo simb√≥lico o una validaci√≥n inicial de integridad, no un error.\n",
    "\n",
    "Dado que no hay datos vac√≠os ni inconsistentes, no se requieren procesos adicionales de limpieza, imputaci√≥n ni eliminaci√≥n de filas o columnas, lo que permite avanzar directamente al an√°lisis exploratorio, transformaci√≥n de caracter√≠sticas o entrenamiento de modelos con confianza en la calidad del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8ce89",
   "metadata": {},
   "source": [
    "## Preparaci√≥n del conjunto de entrenamiento y prueba\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ac0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestManager:\n",
    "  @staticmethod\n",
    "  def stratified_train_test_split(df, label_col, train_ratio : int, seed : int):\n",
    "    \"\"\"\n",
    "    Performs a stratified split of the DataFrame based on the label column.\n",
    "    Returns (train_df, test_df).\n",
    "    \"\"\"\n",
    "    label_values = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    train_fractions = {label: train_ratio for label in label_values}\n",
    "\n",
    "    train_df = df.sampleBy(label_col, train_fractions, seed=seed)\n",
    "\n",
    "    train_ids = train_df.select(F.monotonically_increasing_id().alias(\"id\"))\n",
    "    df_with_id = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    test_df = df_with_id.join(train_ids, on=\"id\", how=\"left_anti\").drop(\"id\")\n",
    "    train_df = train_df.drop(\"id\") if \"id\" in train_df.columns else train_df\n",
    "\n",
    "    print(f\"Train set: {train_df.count()} rows\")\n",
    "    print(f\"Test set: {test_df.count()} rows\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7de65e",
   "metadata": {},
   "source": [
    "## COnstrucci√≥n de conjunto de entrenamiento y prueba\n",
    "---\n",
    "\n",
    "Se realiz√≥ una divisi√≥n estratificada del DataFrame df_sample_M utilizando la clase TrainTestManager. Esto significa que los datos fueron separados en entrenamiento y prueba manteniendo la proporci√≥n original de las clases presentes en la columna \"sentiment\", que es la variable objetivo.\n",
    "\n",
    "\n",
    "Recopila todos los valores √∫nicos de la clase (label_col = \"sentiment\").\n",
    "Aplica un muestreo estratificado con una fracci√≥n de 80% para entrenamiento (train_ratio = 0.8) y el 20% restante para prueba.\n",
    "Usa una semilla fija (seed = 42) para garantizar reproducibilidad.\n",
    "Se asegura de que los datos del conjunto de prueba no se solapen con los del conjunto de entrenamiento.\n",
    "\n",
    "Resultado de la divisi√≥n:\n",
    "\n",
    "Training set: 276,731 muestras (80%)\n",
    "Test set: 69,098 muestras (20%)\n",
    "\n",
    "\n",
    "Este enfoque mejora la generalizaci√≥n del modelo, especialmente cuando se trabaja con clases desbalanceadas, ya que cada clase est√° representada proporcionalmente en ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b16eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 276731 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 69098 rows\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = TrainTestManager.stratified_train_test_split(df_sample_M, \"sentiment\", 0.8, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f6a82",
   "metadata": {},
   "source": [
    "## Selecci√≥n de m√©tricas de calidad\n",
    "---\n",
    "Accuracy:\n",
    ">Accuracy mide la proporci√≥n de predicciones correctas que hace el modelo sobre el total de casos. Es decir, cu√°ntas veces el modelo acierta, sin distinguir entre clases. Es √∫til cuando las clases est√°n balanceadas y los errores tienen el mismo peso. Sin embargo, puede ser enga√±osa si hay desbalance de clases, ya que un modelo puede tener alta exactitud simplemente prediciendo siempre la clase mayoritaria.\n",
    "\n",
    "Precision\n",
    ">La precisi√≥n (precision) es una m√©trica que indica qu√© proporci√≥n de las predicciones positivas realizadas por el modelo fueron correctas. Es especialmente importante en contextos donde los falsos positivos tienen un alto costo, como en sistemas de detecci√≥n de fraude, diagn√≥sticos m√©dicos o filtrado de spam. Una alta precisi√≥n significa que el modelo es confiable al identificar positivos, ya que comete pocos errores al etiquetar como tal. Su importancia radica en que permite evaluar la exactitud del modelo cuando afirma que una instancia pertenece a una clase positiva, ayudando a evitar consecuencias negativas derivadas de decisiones incorrectas.\n",
    "\n",
    "f1_score\n",
    ">F1-score es la media arm√≥nica entre la precisi√≥n (precision) y la exhaustividad (recall). Es una m√©trica clave cuando hay un desbalance entre clases o cuando los falsos positivos y falsos negativos tienen consecuencias importantes. Sirve para tener una visi√≥n m√°s equilibrada del desempe√±o del modelo que usando solo accuracy, ya que castiga los extremos donde una m√©trica es alta y la otra baja.\n",
    "\n",
    "precision\n",
    ">Precision indica qu√© proporci√≥n de las predicciones positivas realmente lo son. Es esencial cuando el costo de un falso positivo es alto. Por ejemplo, si un modelo predice que un producto es defectuoso y no lo es, puede generar gastos innecesarios. Por eso, esta m√©trica eval√∫a qu√© tan \"confiables\" son los aciertos positivos del modelo.\n",
    "\n",
    "recall\n",
    ">Recall (tambi√©n llamado sensibilidad o exhaustividad) mide la capacidad del modelo para identificar todos los casos positivos reales. Es vital cuando los falsos negativos son especialmente graves, como en el diagn√≥stico m√©dico o detecci√≥n de fraudes. Un alto recall asegura que el modelo no est√° dejando pasar demasiados positivos verdaderos sin detectar.\n",
    "\n",
    "ROC AUC\n",
    ">La ROC AUC (Receiver Operating Characteristic - Area Under the Curve) es una m√©trica que eval√∫a la capacidad de un modelo para distinguir entre clases positivas y negativas. La curva ROC compara la tasa de verdaderos positivos (recall) frente a la tasa de falsos positivos a diferentes umbrales de decisi√≥n, y el √°rea bajo esta curva (AUC) resume el rendimiento del modelo en un solo valor entre 0 y 1. Un AUC de 1.0 indica un modelo perfecto, mientras que 0.5 sugiere un modelo sin capacidad predictiva (equivalente a adivinar al azar). Su importancia radica en que es independiente del umbral y √∫til especialmente cuando hay clases desbalanceadas, ya que proporciona una visi√≥n global del comportamiento del modelo m√°s all√° de una √∫nica m√©trica como precisi√≥n o recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb047e",
   "metadata": {},
   "source": [
    "## Construcci√≥n de modelo de aprendizaje\n",
    "--- \n",
    "La clase ManualLogisticRegressionManager se encarga de gestionar todo el proceso necesario para entrenar, preprocesar y evaluar un modelo de regresi√≥n log√≠stica binaria utilizando PySpark. Primero, permite preparar los datos mediante un pipeline que ensambla las variables predictoras y las escala para que tengan una magnitud comparable. Luego, entrena el modelo aplicando validaci√≥n cruzada con una b√∫squeda de hiperpar√°metros (regParam y elasticNetParam) para encontrar la mejor configuraci√≥n posible, evaluando el rendimiento con el √°rea bajo la curva ROC (AUC). Finalmente, eval√∫a el modelo generando m√©tricas clave como la precisi√≥n, recall, F1-score, accuracy y el propio AUC, lo que permite tener una visi√≥n completa del desempe√±o del modelo en tareas de clasificaci√≥n binaria. Esta clase automatiza y organiza eficientemente cada una de las etapas del flujo de trabajo del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "573aa82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLogisticRegressionManager:\n",
    "    @staticmethod\n",
    "    def evaluate_binary_classification(predictions, label_col: str = \"label\", prediction_col: str = \"prediction\", probability_col: str = \"probability\"):\n",
    "        \"\"\"\n",
    "        Eval√∫a un modelo de clasificaci√≥n binaria en base a varias m√©tricas est√°ndar.\n",
    "        \"\"\"\n",
    "        accuracy = predictions.filter(F.col(label_col) == F.col(prediction_col)).count() / predictions.count()\n",
    "        \n",
    "        tp = predictions.filter((F.col(prediction_col) == 1) & (F.col(label_col) == 1)).count()\n",
    "        fp = predictions.filter((F.col(prediction_col) == 1) & (F.col(label_col) == 0)).count()\n",
    "        fn = predictions.filter((F.col(prediction_col) == 0) & (F.col(label_col) == 1)).count()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0.0\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=probability_col, metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            \"accuracy\": [accuracy],\n",
    "            \"precision\": [precision],\n",
    "            \"recall\": [recall],\n",
    "            \"f1_score\": [f1],\n",
    "            \"auc\": [auc]\n",
    "        })\n",
    "\n",
    "        return results_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_data(df_train, df_test, feature_cols, label_col=\"sentiment\"):\n",
    "        \"\"\"\n",
    "        Arma un pipeline para ensamblar y escalar caracter√≠sticas.\n",
    "        \"\"\"\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "        scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withStd=True, withMean=False)\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "        pipeline_model = pipeline.fit(df_train)\n",
    "        train_preprocessed = pipeline_model.transform(df_train)\n",
    "        test_preprocessed = pipeline_model.transform(df_test)\n",
    "\n",
    "        return train_preprocessed, test_preprocessed\n",
    "\n",
    "    @staticmethod\n",
    "    def train_logistic_regression(train_df, test_df, label_col=\"sentiment\"):\n",
    "        \"\"\"\n",
    "        Trains a logistic regression model on the training set and evaluates it on the test set.\n",
    "        Returns the trained model and evaluation metrics.\n",
    "        \"\"\"\n",
    "        lr = LogisticRegression(labelCol=label_col, featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(lr.regParam, [0.1])\n",
    "            .addGrid(lr.elasticNetParam, [0.0])\n",
    "            .build())\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "        cv = CrossValidator(estimator=lr,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=2,\n",
    "                            parallelism=2)\n",
    "\n",
    "        cv_model = cv.fit(train_df)\n",
    "        best_model = cv_model.bestModel\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        print(f\"BEst model: RegParam = {best_model._java_obj.getRegParam()}, ElasticNet = {best_model._java_obj.getElasticNetParam()}\")\n",
    "        \n",
    "        return predictions, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8feafb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features = [\"star_rating\", \"helpful_votes\", \"total_votes\"]\n",
    "\n",
    "train_pp, test_pp = ManualLogisticRegressionManager.preprocess_data(\n",
    "  train_df.sample(withReplacement=False, fraction=0.1, seed=42),\n",
    "  test_df.sample(withReplacement=False, fraction=0.1, seed=42),\n",
    "  feature_cols=features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562ce0a",
   "metadata": {},
   "source": [
    "## An√°lisis de resultados\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26e13d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEst model: RegParam = 0.1, ElasticNet = 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions, best_model = ManualLogisticRegressionManager.train_logistic_regression(train_pp, test_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ee0a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = ManualLogisticRegressionManager.evaluate_binary_classification(predictions, label_col=\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "408b1133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.933079</td>\n",
       "      <td>0.910404</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>0.953014</td>\n",
       "      <td>0.999809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score       auc\n",
       "0  0.933079   0.910404  0.999809  0.953014  0.999809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f077e5c",
   "metadata": {},
   "source": [
    "El modelo de regresi√≥n log√≠stica binaria obtuvo resultados excepcionales seg√∫n las m√©tricas evaluadas, lo que indica un rendimiento altamente confiable y preciso. La exactitud (accuracy) alcanz√≥ un 93.31%, lo que significa que el modelo clasific√≥ correctamente m√°s del 93% de los casos en general, una cifra muy alta que refleja su consistencia global.\n",
    "\n",
    "La precisi√≥n fue de 91.04%, lo que implica que, de todas las instancias que el modelo predijo como positivas, m√°s del 91% realmente lo eran. Esto es especialmente importante en contextos donde los falsos positivos tienen un alto costo, ya que ayuda a reducir errores en decisiones automatizadas.\n",
    "\n",
    "Por otro lado, el recall fue pr√°cticamente perfecto, con un 99.98%, lo que indica que el modelo logr√≥ identificar casi todos los casos positivos reales. Esta m√©trica es crucial en aplicaciones donde los falsos negativos son cr√≠ticos, como en la detecci√≥n de fraudes, diagn√≥sticos m√©dicos o control de calidad.\n",
    "\n",
    "El F1 Score, que combina precisi√≥n y recall en una √∫nica m√©trica arm√≥nica, alcanz√≥ un 95.30%, lo que muestra un excelente balance entre ambas capacidades. Esto refuerza que el modelo no solo es preciso, sino tambi√©n exhaustivo al identificar correctamente las clases.\n",
    "\n",
    "Finalmente, el AUC-ROC fue de 0.9998, una m√©trica que eval√∫a la capacidad del modelo para distinguir entre las clases. Un valor tan cercano a 1 demuestra que el modelo tiene un poder discriminativo casi perfecto, es decir, que puede separar muy bien los positivos de los negativos en diferentes umbrales de decisi√≥n.\n",
    "\n",
    "Tiene un alto desempe√±o y se valida la solidez del modelo para ser implementado en entornos de producci√≥n donde se requiere alta confiabilidad y m√≠nimo margen de error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_4_big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
