{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59188e2",
   "metadata": {},
   "source": [
    "# Actividad 3 | Aprendizaje supervisado y no supervisado\n",
    "---\n",
    "- Alonso Pedrero Mart√≠nez   |   A01769076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbe5b3",
   "metadata": {},
   "source": [
    "## Aprendizaje supervisado y no supervisado\n",
    "---\n",
    "\n",
    "- ‚Äã‚ÄãAprendizaje supervisado: es un enfoque de aprendizaje autom√°tico que se define por el uso de conjuntos de datos etiquetados. Estos conjuntos de datos est√°n dise√±ados para entrenar o supervisar algoritmos para clasificar datos o predecir resultados con precisi√≥n. Mediante entradas y salidas etiquetadas, el modelo puede medir su precisi√≥n y aprender con el tiempo.\n",
    "\n",
    "\n",
    "    - Clasificaci√≥n: los problemas utilizan un algoritmo para asignar con precisi√≥n los datos de prueba a categor√≠as espec√≠ficas, como separar manzanas de naranjas. O, en el mundo real, los algoritmos de aprendizaje supervisado pueden utilizarse para clasificar el correo no deseado en una carpeta separada de la bandeja de entrada. Los clasificadores lineales, las m√°quinas de vectores de soporte, los √°rboles de decisi√≥n y los bosques aleatorios son tipos comunes de algoritmos de clasificaci√≥n.\n",
    "\n",
    "\n",
    "    - Regresi√≥n: es otro tipo de m√©todo de aprendizaje supervisado que utiliza un algoritmo para comprender la relaci√≥n entre las variables dependientes e independientes. Los modelos de regresi√≥n son √∫tiles para predecir valores num√©ricos basados ‚Äã‚Äãen diferentes puntos de datos, como las proyecciones de ingresos por ventas para una empresa determinada. Algunos algoritmos de regresi√≥n populares son la regresi√≥n lineal, la regresi√≥n log√≠stica y la regresi√≥n polin√≥mica.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "\n",
    "- Aprendizaje no supervisado: utiliza algoritmos de aprendizaje autom√°tico para analizar y agrupar conjuntos de datos sin etiquetar. Estos algoritmos descubren patrones ocultos en los datos sin necesidad de intervenci√≥n humana.\n",
    "\n",
    "\n",
    "    - Agrupamiento: es una t√©cnica de miner√≠a de datos que permite agrupar datos sin etiquetar en funci√≥n de sus similitudes o diferencias. Por ejemplo, los algoritmos de agrupamiento K-medias asignan puntos de datos similares a grupos, donde el valor K representa el tama√±o de la agrupaci√≥n y la granularidad. Esta t√©cnica es √∫til para la segmentaci√≥n de mercado, la compresi√≥n de im√°genes, etc.\n",
    "\n",
    "\n",
    "    - Asociaci√≥n: es otro tipo de m√©todo de aprendizaje no supervisado que utiliza diferentes reglas para encontrar relaciones entre las variables de un conjunto de datos determinado. Estos m√©todos se utilizan con frecuencia para el an√°lisis de la cesta de la compra y los motores de recomendaci√≥n, como las recomendaciones de \"Los clientes que compraron este art√≠culo tambi√©n compraron\".\n",
    "\n",
    "\n",
    "    - Reducci√≥n de la dimensionalidad: es una t√©cnica de aprendizaje que se utiliza cuando el n√∫mero de caracter√≠sticas (o dimensiones) en un conjunto de datos determinado es demasiado elevado. Reduce la cantidad de datos de entrada a un tama√±o manejable, preservando al mismo tiempo su integridad. Esta t√©cnica se utiliza a menudo en la etapa de preprocesamiento de datos, como cuando los autocodificadores eliminan el ruido de los datos visuales para mejorar la calidad de la imagen.\n",
    "\n",
    "--- \n",
    "Referencia bibliogr√°fica:\n",
    "\n",
    "    Delua, J. (2025, April 17). Supervised vs unsupervised learning. Supervised versus unsupervised learning: What‚Äôs the difference? https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ed20b",
   "metadata": {},
   "source": [
    "## Aprendizaje supervisado y no supervisado en pySpark\n",
    "---\n",
    "üîπ Modelos de Aprendizaje Supervisado en PySpark\n",
    "\n",
    "üî∏ Clasificaci√≥n\n",
    "\n",
    "- LogisticRegression ‚Äì Regresi√≥n log√≠stica binaria o multinomial\n",
    "- DecisionTreeClassifier ‚Äì √Årbol de decisi√≥n para clasificaci√≥n\n",
    "- RandomForestClassifier ‚Äì Bosques aleatorios\n",
    "- GBTClassifier ‚Äì Gradient-Boosted Trees\n",
    "- NaiveBayes ‚Äì Clasificador basado en teorema de Bayes\n",
    "- MultilayerPerceptronClassifier ‚Äì Red neuronal multicapa\n",
    "- LinearSVC ‚Äì M√°quina de vectores de soporte lineal\n",
    "\n",
    "\n",
    "üî∏ Regresi√≥n\n",
    "\n",
    "- LinearRegression ‚Äì Regresi√≥n lineal\n",
    "- DecisionTreeRegressor ‚Äì √Årbol de decisi√≥n para regresi√≥n\n",
    "- RandomForestRegressor ‚Äì Bosques aleatorios\n",
    "- GBTRegressor ‚Äì Gradient-Boosted Trees\n",
    "- GeneralizedLinearRegression ‚Äì Regresi√≥n generalizada (Poisson, Binomial, etc.)\n",
    "- AFTSurvivalRegression ‚Äì An√°lisis de supervivencia\n",
    "\n",
    "\n",
    "üîπ Modelos de Aprendizaje No Supervisado en PySpark\n",
    "\n",
    "üî∏ Clustering\n",
    "\n",
    "- KMeans ‚Äì Clustering por K-medias\n",
    "- GaussianMixture ‚Äì Modelos de mezcla gaussiana (soft clustering)\n",
    "- BisectingKMeans ‚Äì Variante jer√°rquica de K-means\n",
    "- LDA ‚Äì An√°lisis de Dirichlet latente (para temas en texto)\n",
    "\n",
    "üî∏ Reducci√≥n de Dimensionalidad\n",
    "\n",
    "- PCA ‚Äì An√°lisis de Componentes Principales\n",
    "- SVD ‚Äì Descomposici√≥n en valores singulares (indirectamente a trav√©s de ALS y PCA)\n",
    "\n",
    "üîπ Otros Algoritmos √ötiles\n",
    "- ALS ‚Äì Alternating Least Squares (recomendadores colaborativos)\n",
    "- IsotonicRegression ‚Äì Regresi√≥n isot√≥nica (no lineal y no param√©trica)\n",
    "- OneVsRest ‚Äì Envoltorio para clasificaci√≥n multiclase\n",
    "- Pipeline ‚Äì Para encadenar transformaciones y modelos\n",
    "- CrossValidator y TrainValidationSplit ‚Äì Validaci√≥n cruzada y tuning de hiperpar√°metros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d27a9",
   "metadata": {},
   "source": [
    "## Selecci√≥n de los datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "47c68fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from pyspark.sql.types import StringType, DoubleType, FloatType\n",
    "import findspark\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bf7ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cesarivp/Documents/GitHub/big_data/myenv/lib/python3.13/site-packages/pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d76b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/25 10:48:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.117:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10dc181a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4910d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../files\"\n",
    "FILE = \"amazon_electronics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d85f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    @staticmethod\n",
    "    def open_csv_file(input_path : str, file_name : str):\n",
    "        \"\"\"\n",
    "        This method opens a csv file with pyspark\n",
    "        \"\"\"\n",
    "        csv_df = spark.read.csv(\n",
    "            path.join(input_path, file_name),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            multiLine=True,\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\"\n",
    "        )\n",
    "\n",
    "        csv_df.show(truncate=20)\n",
    "\n",
    "        return csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac930c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   22873041|R3ARRMDEGED8RD|B00KJWQIIC|     335625766|Plemo 14-Inch Lap...|              PC|          5|            0|          0|   N|                Y|Pleasantly surprised|I was very surpri...| 2015-08-31|        1|\n",
      "|         US|   30088427| RQ28TSA020Y6J|B013ALA9LA|     671157305|TP-Link OnHub AC1...|              PC|          5|           24|         31|   N|                N|OnHub is a pretty...|I am a Google emp...| 2015-08-31|        1|\n",
      "|         US|   20329786| RUXJRZCT6953M|B00PML2GQ8|     982036237|AmazonBasics USB ...|              PC|          1|            2|          2|   N|                N|None of them work...|Bought cables in ...| 2015-08-31|        0|\n",
      "|         US|   14215710| R7EO0UO6BPB71|B001NS0OZ4|     576587596|Transcend P8 15-i...|              PC|          1|            0|          0|   N|                Y|just keep searching.|nope, cheap and slow| 2015-08-31|        0|\n",
      "|         US|   38264512|R39NJY2YJ1JFSV|B00AQMTND2|     964759214|Aleratec SATA Dat...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Excellent! Great ...| 2015-08-31|        1|\n",
      "|         US|   30548466|R31SR7REWNX7CF|B00KX4TORI|     170101802|Kingston Digital ...|              PC|          5|            0|          0|   N|                Y|Good quality, wor...|Good quality,work...| 2015-08-31|        1|\n",
      "|         US|     589298| RVBP8I1R0CTZ8|B00P17WEMY|     206124740|White 9 Inch Unlo...|              PC|          3|            1|          2|   N|                Y|in fact this is t...|This demn tablet ...| 2015-08-31|        0|\n",
      "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|        1|\n",
      "|         US|   50728290|R23AICGEDAJQL1|B0098Y77OG|     177098042|                Acer|              PC|          1|            0|          0|   N|                Y|You get what you ...|After exactly 45 ...| 2015-08-31|        0|\n",
      "|         US|   37802374|R2EY3N4K9W19UP|B00IFYEYXC|     602496520|AzureWave Broadco...|              PC|          5|            3|          4|   N|                Y|Great for Windows...|Replaced my Intel...| 2015-08-31|        1|\n",
      "|         US|   52027882| RC9AW4HKJ016M|B0091ITP0S|     977217357|HDE Rotating iPad...|              PC|          1|            0|          0|   N|                Y|            One Star|IT HAS ALREADY CR...| 2015-08-31|        0|\n",
      "|         US|   41770239|R2ALWJE9N6ZBXD|B008I21EA2|     295632907|Linksys AC1750 Wi...|              PC|          1|            0|          0|   N|                N|   Very Disappointed|Very disappointed...| 2015-08-31|        0|\n",
      "|         US|   42560427|R2G5FPA4OX37GV|B00MRB7SBO|     922591915|iPad Pro 9.7, iPa...|              PC|          5|            1|          1|   N|                Y|          Five Stars|Works well. I use...| 2015-08-31|        1|\n",
      "|         US|   46345923|R1IKTSEVXSIMOD|B00LLER2CS|     997551273|SanDisk 16GB CZ43...|              PC|          5|            0|          0|   N|                Y|The encryption so...|The encryption so...| 2015-08-31|        1|\n",
      "|         US|   41751192|R2YA6G6SRFEWF6|B00B0CQCCC|     937999925|TRENDnet Wireless...|              PC|          1|            0|          1|   N|                Y|Didn't last 2 years.|I have owned this...| 2015-08-31|        0|\n",
      "|         US|   21176481| RS9H1N9I3Z1IA|B00GU8W5AE|      13865167|Redragon M901 PER...|              PC|          5|            0|          0|   N|                Y|Awesome gaming mouse|My first gaming m...| 2015-08-31|        1|\n",
      "|         US|   10674058| RKKLBI76VTDNT|B00XHMXJQ0|     967483469|Mudder MHL Adapte...|              PC|          1|            0|          0|   N|                Y|            One Star|I cannot get it t...| 2015-08-31|        0|\n",
      "|         US|   43341796|R2NJ3WFUS4E5G6|B00YGJJQ6U|     986548413|Fintie iPad Air 2...|              PC|          4|            0|          0|   N|                Y|Great choices of ...|Love that Finite ...| 2015-08-31|        1|\n",
      "|         US|   13232866|R21PTQNLGCBN0I|B00XMN20Y6|     873354048|Fintie iPad 2/3/4...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Nice color, I lov...| 2015-08-31|        1|\n",
      "|         US|   29333557|R3G4RT3EQ9RSY7|B00MA40W9I|     535866197|Egoway¬Æ New Lapto...|              PC|          1|            0|          0|   N|                Y|Totally wasted $6...|Totally wasted $6...| 2015-08-31|        0|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_reviews = FileManager.open_csv_file(PATH, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e39c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Previously defined relevant columns for the activity.\n",
    "RELEVANT_COLUMNS_FOR_CHARACTERIZATION = [\n",
    "  \"star_rating\",\n",
    "  \"helpful_votes\",\n",
    "  \"total_votes\",\n",
    "  \"vine\",\n",
    "  \"verified_purchase\",\n",
    "  \"review_date\",\n",
    "  \"sentiment\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7e3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_filtered = df_reviews.select(*RELEVANT_COLUMNS_FOR_CHARACTERIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507d63d",
   "metadata": {},
   "source": [
    "### Generaci√≥n de las particiones\n",
    "\n",
    "Se implement√≥ un procedimiento autom√°tico en PySpark que:\n",
    "\n",
    "1. Filtra los registros de la base de datos que cumplen con cada combinaci√≥n de valores.\n",
    "2. Almacena cada subconjunto en un diccionario indexado por nombre de combinaci√≥n (ej. \"R5_VPY_VN\" para `star_rating`=5, `verified_purchase`=Y, `vine`=N).\n",
    "3. Imprime la cantidad de registros por partici√≥n para control y trazabilidad.\n",
    "\n",
    "Las particiones con muy pocos registros pueden ser descartadas en etapas posteriores para evitar problemas en el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad608743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitioningManager:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_probabilities(df, cols):\n",
    "        \"\"\"\n",
    "        Computes and returns the probability of each combination of values in the specified columns.\n",
    "        \"\"\"\n",
    "        total_count = df.count()\n",
    "        return df.groupBy(cols).count() \\\n",
    "                 .withColumn(\"probability\", F.round(F.col(\"count\") / total_count, 6)) \\\n",
    "                 .orderBy(\"probability\", ascending=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_partition(df, star_rating, verified_purchase, vine):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame by specific values for rating, verified purchase, and vine.\n",
    "        \"\"\"\n",
    "        return df.filter(\n",
    "            (F.col(\"star_rating\") == star_rating) &\n",
    "            (F.col(\"verified_purchase\") == verified_purchase) &\n",
    "            (F.col(\"vine\") == vine)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_all_partitions(df, min_probability=0.0001):\n",
    "        \"\"\"\n",
    "        Generates partitions only for combinations whose joint probability is above min_probability.\n",
    "        \"\"\"\n",
    "\n",
    "        prob_df = PartitioningManager.compute_probabilities(\n",
    "            df, [\"star_rating\", \"verified_purchase\", \"vine\"]\n",
    "        )\n",
    "\n",
    "        filtered_combinations = prob_df.filter(\n",
    "            F.col(\"probability\") >= min_probability\n",
    "        ).select(\"star_rating\", \"verified_purchase\", \"vine\").collect()\n",
    "\n",
    "        partitions = {}\n",
    "        for row in filtered_combinations:\n",
    "            rating = row[\"star_rating\"]\n",
    "            purchase = row[\"verified_purchase\"]\n",
    "            vine = row[\"vine\"]\n",
    "\n",
    "            key = f\"R{rating}_VP{purchase}_V{vine}\"\n",
    "            filtered = PartitioningManager.filter_partition(df, rating, purchase, vine)\n",
    "            partitions[key] = filtered\n",
    "            print(f\"Partition {key} created with {filtered.count()} records.\")\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def stratified_sample_partitioned_data(partitions_dict, label_col=\"sentiment\", fraction=0.3, min_rows=50):\n",
    "        \"\"\"\n",
    "        Applies stratified sampling to each partition based on sentiment.\n",
    "        \"\"\"\n",
    "        sampled_partitions = {}\n",
    "\n",
    "        for key, df in partitions_dict.items():\n",
    "            count = df.count()\n",
    "\n",
    "            if count < min_rows:\n",
    "                print(f\"Skipping partition {key} ‚Äî only {count} rows (<{min_rows})\")\n",
    "                continue\n",
    "\n",
    "            sentiments = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            fractions = {s: fraction for s in sentiments}\n",
    "\n",
    "            sampled_df = df.sampleBy(label_col, fractions, seed=42)\n",
    "            sampled_partitions[key] = sampled_df\n",
    "            print(f\"Sampled {sampled_df.count()} rows from partition {key} (original: {count})\")\n",
    "\n",
    "        return sampled_partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def build_combined_sample(partitions_sampled_dict):\n",
    "        \"\"\"\n",
    "        Unites all sampled partitions into a single DataFrame (M).\n",
    "        This helps reduce computational load while maintaining diversity.\n",
    "        \"\"\"\n",
    "        if not partitions_sampled_dict:\n",
    "            raise ValueError(\"No partitions provided for sample combination.\")\n",
    "\n",
    "        combined_df = None\n",
    "        for key, df in partitions_sampled_dict.items():\n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = combined_df.union(df)\n",
    "            print(f\"Partition {key} added to the combined sample.\")\n",
    "\n",
    "        print(f\"Total records in combined sample: {combined_df.count()}\")\n",
    "        return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18dbca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN created with 3679909 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPY_VN created with 1019728 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPY_VN created with 603371 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPY_VN created with 443364 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VN created with 410073 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPY_VN created with 300544 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VN created with 152779 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VN created with 135197 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VN created with 65398 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VN created with 59973 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VY created with 15604 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VY created with 13240 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VY created with 4886 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VY created with 1634 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VY created with 705 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VY created with 101 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "partitions = PartitioningManager.generate_all_partitions(df_reviews, min_probability=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec01c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|        1|\n",
      "|         US|   43341796|R2NJ3WFUS4E5G6|B00YGJJQ6U|     986548413|Fintie iPad Air 2...|              PC|          4|            0|          0|   N|                Y|Great choices of ...|Love that Finite ...| 2015-08-31|        1|\n",
      "|         US|   12246861|R2WV71J88ERO3H|B00J082KFQ|      55148922|Case Logic BPCA-1...|              PC|          4|            0|          0|   N|                Y|  Slimmer laptop bag|This is pretty mu...| 2015-08-31|        1|\n",
      "|         US|   23531656|R1DG8L9CSXXPJK|B008IFXQFU|     422480840|TP- Link Wireless...|              PC|          4|            0|          0|   N|                Y|          Four Stars|It works for its ...| 2015-08-31|        1|\n",
      "|         US|   28827675|R1PHDY2F126NRD|B00MB80WE8|     763447327|Prontotec Axius N...|              PC|          4|            0|          0|   N|                Y|So affordable & w...|This was bought f...| 2015-08-31|        1|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#¬†Sample of one of the generated partitions.\n",
    "partitions[\"R4_VPY_VN\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ba004",
   "metadata": {},
   "source": [
    "## 3.T√©cnica de muestreo aplicada por partici√≥n\n",
    "\n",
    "Una vez construidas las particiones, se aplic√≥ una t√©cnica de **muestreo estratificado** sobre cada subconjunto, usando la variable `sentiment` como variable de estratificaci√≥n. Esto asegura que cada muestra mantenga la proporci√≥n original de clases de sentimiento en la partici√≥n.\n",
    "\n",
    "Para evitar particiones con tama√±os insuficientes, se defini√≥ un umbral m√≠nimo (`min_rows`) que descarta autom√°ticamente las particiones con muy pocos registros.\n",
    "\n",
    "Adem√°s, se permiti√≥ configurar el porcentaje de muestreo (`fraction`) por clase de sentimiento. Esto ofrece flexibilidad para ajustar el tama√±o del conjunto de entrenamiento o validaci√≥n seg√∫n necesidades posteriores.\n",
    "\n",
    "#### Justificaci√≥n del muestreo estratificado\n",
    "\n",
    "* **Preservaci√≥n del equilibrio de clases**: Al muestrear por clase de sentimiento se evitan sesgos por clases desbalanceadas.\n",
    "* **Relevancia contextual**: Al aplicar el muestreo dentro de cada partici√≥n (y no sobre la base completa), se conserva la variabilidad contextual de las rese√±as.\n",
    "* **Evita el submuestreo accidental**: Las particiones muy peque√±as son descartadas de forma controlada, garantizando que el conjunto final tenga representatividad suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb6bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 184082 rows from partition R5_VPY_VN (original: 3679909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 51148 rows from partition R4_VPY_VN (original: 1019728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 30183 rows from partition R1_VPY_VN (original: 603371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 22223 rows from partition R3_VPY_VN (original: 443364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 20507 rows from partition R5_VPN_VN (original: 410073)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 15034 rows from partition R2_VPY_VN (original: 300544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 7595 rows from partition R1_VPN_VN (original: 152779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 6708 rows from partition R4_VPN_VN (original: 135197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3345 rows from partition R3_VPN_VN (original: 65398)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3066 rows from partition R2_VPN_VN (original: 59973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 817 rows from partition R5_VPN_VY (original: 15604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 724 rows from partition R4_VPN_VY (original: 13240)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 266 rows from partition R3_VPN_VY (original: 4886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 93 rows from partition R2_VPN_VY (original: 1634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 35 rows from partition R1_VPN_VY (original: 705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3 rows from partition R5_VPY_VY (original: 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_partitions = PartitioningManager.stratified_sample_partitioned_data(partitions, fraction=0.05, min_rows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e764fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN added to the combined sample.\n",
      "Partition R4_VPY_VN added to the combined sample.\n",
      "Partition R1_VPY_VN added to the combined sample.\n",
      "Partition R3_VPY_VN added to the combined sample.\n",
      "Partition R5_VPN_VN added to the combined sample.\n",
      "Partition R2_VPY_VN added to the combined sample.\n",
      "Partition R1_VPN_VN added to the combined sample.\n",
      "Partition R4_VPN_VN added to the combined sample.\n",
      "Partition R3_VPN_VN added to the combined sample.\n",
      "Partition R2_VPN_VN added to the combined sample.\n",
      "Partition R5_VPN_VY added to the combined sample.\n",
      "Partition R4_VPN_VY added to the combined sample.\n",
      "Partition R3_VPN_VY added to the combined sample.\n",
      "Partition R2_VPN_VY added to the combined sample.\n",
      "Partition R1_VPN_VY added to the combined sample.\n",
      "Partition R5_VPY_VY added to the combined sample.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 219:============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in combined sample: 345829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sample_M = PartitioningManager.build_combined_sample(sampled_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792c0d5",
   "metadata": {},
   "source": [
    "## Preparaci√≥n del conjunto de entrenamiento y prueba\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "897e09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalAnalysisHelper():\n",
    "    @staticmethod\n",
    "    def dataset_dimensions(df_input):\n",
    "        print(\"columns in the dataset:\", len(df_input.columns))\n",
    "        print(\"rows in the dataset:\", df_input.count())\n",
    "\n",
    "    @staticmethod\n",
    "    def schema_information(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the current schema of the data.\n",
    "        \"\"\"\n",
    "        df_input.printSchema()\n",
    "\n",
    "    @staticmethod\n",
    "    def descriptive_statistics(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the descriptive statistics of the data.\n",
    "        \"\"\"\n",
    "        df_input.summary().show(truncate=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def missing_values_table(df_input):\n",
    "        \"\"\"\n",
    "        Displays a table with the count of missing values per column.\n",
    "        \"\"\"\n",
    "        missing_exprs = []\n",
    "        \n",
    "        for c in df_input.schema.fields:\n",
    "            field_name = c.name\n",
    "            field_type = c.dataType\n",
    "            \n",
    "            if isinstance(field_type, (DoubleType, FloatType)):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | isnan(col(field_name)), field_name)).alias(field_name)\n",
    "                )\n",
    "            elif isinstance(field_type, StringType):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | (col(field_name) == \"\"), field_name)).alias(field_name)\n",
    "                )\n",
    "            else:\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull(), field_name)).alias(field_name)\n",
    "                )\n",
    "\n",
    "        df_missing_values = df_input.select(missing_exprs)\n",
    "\n",
    "        return df_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b228315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in the dataset: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 222:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows in the dataset: 345829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.dataset_dimensions(df_sample_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8615d944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: integer (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.schema_information(df_sample_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc7f9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 225:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|summary|marketplace|customer_id         |review_id     |product_id         |product_parent      |product_title                                                                                 |product_category|star_rating       |helpful_votes     |total_votes       |vine  |verified_purchase|review_headline                   |review_body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |sentiment          |\n",
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|count  |345829     |345829              |345829        |345829             |345829              |345829                                                                                        |345829          |345829            |345829            |345829            |345829|345829           |345829                            |345829                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |345829             |\n",
      "|mean   |NULL       |2.7991213281448346E7|NULL          |7.765654862257862E9|4.9180527983244896E8|1784.6153846153845                                                                            |NULL            |4.086025174291341 |1.4509280598214724|1.9227595140951164|NULL  |NULL             |696.6666666666666                 |3.75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0.763351251630141  |\n",
      "|stddev |NULL       |1.5591539570539333E7|NULL          |3.572692350202524E9|2.8897488575035936E8|1583.630463887913                                                                             |NULL            |1.3622617427344736|46.442456581791674|49.08379117291993 |NULL  |NULL             |1254.9528543601416                |4.272001872658765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |0.42502545879407655|\n",
      "|min    |US         |10157               |R1000CCCY44YL4|0132793040         |1602                |#1080CC Pelican HardBack Case (with Computer Case Liner)                                      |PC              |1                 |0                 |0                 |N     |N                |\u001a\u001a\u001a\u001a\u001a\u001a                            |\u001d\u001dWhile the texture and feel of this keyboard iPad case feels pretty good, I have to agree with another Vine reviewer that the keyboard is awkward. While the keyboard is about the size of a regular wireless Apple keyboard, the keys seem to be moved a little to the left, so it doesn't feel like normal typing. Even more problematic is when you click the space key, your thumb hits up against the edge of the case. So the basic poor design of the keyboard makes it not comfortable for typing, unfortunately.|0                  |\n",
      "|25%    |NULL       |14401893            |NULL          |6.465354276E9      |242889999           |700.0                                                                                         |NULL            |4                 |0                 |0                 |NULL  |NULL             |5.0                               |1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|50%    |NULL       |26724274            |NULL          |9.875971812E9      |488442804           |700.0                                                                                         |NULL            |5                 |0                 |0                 |NULL  |NULL             |7.0                               |1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|75%    |NULL       |42723957            |NULL          |9.966634975E9      |743709280           |3911.0                                                                                        |NULL            |5                 |1                 |1                 |NULL  |NULL             |1333.0                            |3.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|max    |US         |53096561            |RZZZD8HUINFU1 |B01MU3GE5L         |999982788           |‚òÜ 4 Port Powered USB 3.0 Hub Portable for MacBook Air, Windows 8 Tablet PC ‚òÜ Multiple Splitter|PC              |5                 |24714             |26143             |Y     |Y                |üòí Worst purchase I've made online|üòçüëçüëç                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |1                  |\n",
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.descriptive_statistics(df_sample_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c34cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 228:================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "|marketplace|customer_id|review_id|product_id|product_parent|product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|review_headline|review_body|review_date|sentiment|\n",
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "|0          |0          |0        |0         |0             |0            |0               |0          |0            |0          |0   |0                |0              |0          |0          |0        |\n",
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_values = StatisticalAnalysisHelper.missing_values_table(df_sample_M)\n",
    "missing_values.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8ce89",
   "metadata": {},
   "source": [
    "## Preparaci√≥n del conjunto de entrenamiento y prueba\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70ac0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestManager:\n",
    "  @staticmethod\n",
    "  def stratified_train_test_split(df, label_col=\"sentiment\", train_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Performs a stratified split of the DataFrame based on the label column.\n",
    "    Returns (train_df, test_df).\n",
    "    \"\"\"\n",
    "    label_values = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    train_fractions = {label: train_ratio for label in label_values}\n",
    "\n",
    "    train_df = df.sampleBy(label_col, train_fractions, seed=seed)\n",
    "\n",
    "    train_ids = train_df.select(F.monotonically_increasing_id().alias(\"id\"))\n",
    "    df_with_id = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    test_df = df_with_id.join(train_ids, on=\"id\", how=\"left_anti\").drop(\"id\")\n",
    "    train_df = train_df.drop(\"id\") if \"id\" in train_df.columns else train_df\n",
    "\n",
    "    print(f\"Train set: {train_df.count()} rows\")\n",
    "    print(f\"Test set: {test_df.count()} rows\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b16eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 276731 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 891:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 69098 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df = TrainTestManager.stratified_train_test_split(df_sample_M, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb047e",
   "metadata": {},
   "source": [
    "## Construcci√≥n de modelos de aprendizaje supervisado y no supervisado\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "573aa82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    @staticmethod\n",
    "    def train_logistic_regression(train_df, test_df, label_col=\"sentiment\"):\n",
    "        \"\"\"\n",
    "        Trains a logistic regression model on the training set and evaluates it on the test set.\n",
    "        Returns the trained model and evaluation metrics.\n",
    "        \"\"\"\n",
    "        categorical_cols = [\"verified_purchase\", \"vine\"]\n",
    "        numeric_cols = [\"helpful_votes\", \"total_votes\"]\n",
    "\n",
    "        indexers = [\n",
    "            StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\")\n",
    "            for col in categorical_cols\n",
    "        ]\n",
    "\n",
    "        feature_cols = [f\"{col}_idx\" for col in categorical_cols] + numeric_cols\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "\n",
    "        scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\")\n",
    "\n",
    "        lr = LogisticRegression(labelCol=label_col, featuresCol=\"features\", maxIter=20)\n",
    "\n",
    "        pipeline = Pipeline(stages=indexers + [assembler, scaler, lr])\n",
    "        model = pipeline.fit(train_df)\n",
    "\n",
    "        predictions = model.transform(test_df)\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=label_col,\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "        return model, accuracy, predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def train_random_forest(train_df, test_df, label_col=\"sentiment\", num_trees=50):\n",
    "        \"\"\"\n",
    "        Trains a Random Forest classifier on the training set and evaluates it.\n",
    "        Returns the trained model, accuracy score, and predictions.\n",
    "        \"\"\"\n",
    "        categorical_cols = [\"verified_purchase\", \"vine\"]\n",
    "        numeric_cols = [\"helpful_votes\", \"total_votes\"]\n",
    "\n",
    "        indexers = [\n",
    "            StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\")\n",
    "            for col in categorical_cols\n",
    "        ]\n",
    "\n",
    "        feature_cols = [f\"{col}_idx\" for col in categorical_cols] + numeric_cols\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "\n",
    "        rf = RandomForestClassifier(\n",
    "            labelCol=label_col,\n",
    "            featuresCol=\"assembled_features\",\n",
    "            numTrees=num_trees,\n",
    "            maxDepth=10,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        pipeline = Pipeline(stages=indexers + [assembler, rf])\n",
    "        model = pipeline.fit(train_df)\n",
    "\n",
    "        predictions = model.transform(test_df)\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=label_col,\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "        return model, accuracy, predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_kmeans(train_df, test_df, k=2):\n",
    "        categorical_cols = [\"verified_purchase\", \"vine\"]\n",
    "        numeric_cols = [\"helpful_votes\", \"total_votes\"]\n",
    "\n",
    "        indexers = [\n",
    "            StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\")\n",
    "            for col in categorical_cols\n",
    "        ]\n",
    "\n",
    "        # Text processing\n",
    "        tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"tokens\")\n",
    "        remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "        hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
    "        idf = IDF(inputCol=\"raw_features\", outputCol=\"text_features\")\n",
    "\n",
    "        feature_cols = [f\"{col}_idx\" for col in categorical_cols] + numeric_cols + [\"text_features\"]\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "        scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\")\n",
    "\n",
    "        kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=k, seed=42)\n",
    "\n",
    "        pipeline = Pipeline(stages=indexers + [tokenizer, remover, hashingTF, idf, assembler, scaler, kmeans])\n",
    "        model = pipeline.fit(train_df)\n",
    "\n",
    "        test_predictions = model.transform(test_df)\n",
    "\n",
    "        evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\", metricName=\"silhouette\")\n",
    "        accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "        return model, accuracy\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_gmm(train_df, test_df, k=2):\n",
    "        \"\"\"\n",
    "        Trains a Gaussian Mixture Model on the training set and evaluates it on the test set.\n",
    "        Returns the trained model and the silhouette score (as accuracy).\n",
    "        \"\"\"\n",
    "        categorical_cols = [\"verified_purchase\", \"vine\"]\n",
    "        numeric_cols = [\"helpful_votes\", \"total_votes\"]\n",
    "\n",
    "        indexers = [\n",
    "            StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\")\n",
    "            for col in categorical_cols\n",
    "        ]\n",
    "\n",
    "        feature_cols = [f\"{col}_idx\" for col in categorical_cols] + numeric_cols\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "        scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\")\n",
    "\n",
    "        gmm = GaussianMixture(featuresCol=\"features\", predictionCol=\"cluster\", k=k, seed=42)\n",
    "\n",
    "        pipeline = Pipeline(stages=indexers + [assembler, scaler, gmm])\n",
    "        model = pipeline.fit(train_df)\n",
    "\n",
    "        test_predictions = model.transform(test_df)\n",
    "\n",
    "        evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\", metricName=\"silhouette\")\n",
    "        accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "        return model, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model, lr_accuracy, lr_predictions = ModelManager.train_logistic_regression(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542daa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.786231149960925"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Logistic Regression accuracy: {round(lr_accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7947a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 970:================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+---------+\n",
      "|     review_id|prediction|         probability|sentiment|\n",
      "+--------------+----------+--------------------+---------+\n",
      "|R38YS6F8AIPTWS|       1.0|[0.20178237845048...|        1|\n",
      "| RLKRLZ7VRDTGD|       1.0|[0.25605214422464...|        1|\n",
      "|R3CMHUMRLBFRGU|       1.0|[0.20178237845048...|        1|\n",
      "| RL5A9G7ATCTKW|       1.0|[0.20178237845048...|        1|\n",
      "|R2WRS7LOHTEWC1|       1.0|[0.34326285631962...|        1|\n",
      "|R30FVKNS7815YT|       1.0|[0.20178237845048...|        1|\n",
      "| RIG8ZUVB49HVN|       1.0|[0.19742101500703...|        1|\n",
      "|R2JTOYUS285KME|       1.0|[0.20178237845048...|        1|\n",
      "| RRFWALHZISB9T|       1.0|[0.26659069851519...|        1|\n",
      "|R39DPWJB2WS35W|       1.0|[0.20178237845048...|        1|\n",
      "+--------------+----------+--------------------+---------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model.transform(test_df).select(\"review_id\", \"prediction\", \"probability\", \"sentiment\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1933fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 13:05:02 WARN DAGScheduler: Broadcasting large task binary with size 1005.5 KiB\n",
      "25/05/25 13:05:03 WARN DAGScheduler: Broadcasting large task binary with size 1165.7 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fr_model, fr_accuracy, fr_predictions = ModelManager.train_random_forest(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "602634f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy: 0.7825\n"
     ]
    }
   ],
   "source": [
    "print(f\"Random Forest accuracy: {round(fr_accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50dc7ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1022:===============================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+---------+\n",
      "|     review_id|prediction|         probability|sentiment|\n",
      "+--------------+----------+--------------------+---------+\n",
      "|R38YS6F8AIPTWS|       1.0|[0.19324904377050...|        1|\n",
      "| RLKRLZ7VRDTGD|       1.0|[0.32877604441972...|        1|\n",
      "|R3CMHUMRLBFRGU|       1.0|[0.19324904377050...|        1|\n",
      "| RL5A9G7ATCTKW|       1.0|[0.19324904377050...|        1|\n",
      "|R2WRS7LOHTEWC1|       1.0|[0.42131016696662...|        1|\n",
      "|R30FVKNS7815YT|       1.0|[0.19324904377050...|        1|\n",
      "| RIG8ZUVB49HVN|       1.0|[0.20900236205237...|        1|\n",
      "|R2JTOYUS285KME|       1.0|[0.19324904377050...|        1|\n",
      "| RRFWALHZISB9T|       1.0|[0.30358284024919...|        1|\n",
      "|R39DPWJB2WS35W|       1.0|[0.19324904377050...|        1|\n",
      "+--------------+----------+--------------------+---------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fr_model.transform(test_df).select(\"review_id\", \"prediction\", \"probability\", \"sentiment\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11091401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kmeans_model, kmeans_accuracy = ModelManager.train_kmeans(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "266118eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans accuracy: 0.8874\n"
     ]
    }
   ],
   "source": [
    "print(f\"KMeans accuracy: {round(kmeans_accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3f19dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 13:56:02 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gmm_model, gmm_accuracy = ModelManager.train_gmm(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47563371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Mixture accuracy: 0.9557\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gaussian Mixture accuracy: {round(gmm_accuracy, 4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
