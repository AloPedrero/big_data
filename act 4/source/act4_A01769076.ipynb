{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59188e2",
   "metadata": {},
   "source": [
    "# Actividad 4 | Métricas de calidad de resultados\n",
    "---\n",
    "- Alonso Pedrero Martínez   |   A01769076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d27a9",
   "metadata": {},
   "source": [
    "## Selección de los datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47c68fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from pyspark.sql.types import StringType, DoubleType, FloatType\n",
    "import findspark\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import pandas as pd\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56bf7ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alonsopedreromartinez/Documents/GitHub/big_data/act 4/act_4_big_data/lib/python3.12/site-packages/pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d76b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/08 22:20:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x122b38710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4910d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../files\"\n",
    "FILE = \"amazon_electronics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d85f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    @staticmethod\n",
    "    def open_csv_file(input_path : str, file_name : str):\n",
    "        \"\"\"\n",
    "        This method opens a csv file with pyspark\n",
    "        \"\"\"\n",
    "        csv_df = spark.read.csv(\n",
    "            path.join(input_path, file_name),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            multiLine=True,\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\"\n",
    "        )\n",
    "\n",
    "        csv_df.show(truncate=20)\n",
    "\n",
    "        return csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac930c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   22873041|R3ARRMDEGED8RD|B00KJWQIIC|     335625766|Plemo 14-Inch Lap...|              PC|          5|            0|          0|   N|                Y|Pleasantly surprised|I was very surpri...| 2015-08-31|        1|\n",
      "|         US|   30088427| RQ28TSA020Y6J|B013ALA9LA|     671157305|TP-Link OnHub AC1...|              PC|          5|           24|         31|   N|                N|OnHub is a pretty...|I am a Google emp...| 2015-08-31|        1|\n",
      "|         US|   20329786| RUXJRZCT6953M|B00PML2GQ8|     982036237|AmazonBasics USB ...|              PC|          1|            2|          2|   N|                N|None of them work...|Bought cables in ...| 2015-08-31|        0|\n",
      "|         US|   14215710| R7EO0UO6BPB71|B001NS0OZ4|     576587596|Transcend P8 15-i...|              PC|          1|            0|          0|   N|                Y|just keep searching.|nope, cheap and slow| 2015-08-31|        0|\n",
      "|         US|   38264512|R39NJY2YJ1JFSV|B00AQMTND2|     964759214|Aleratec SATA Dat...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Excellent! Great ...| 2015-08-31|        1|\n",
      "|         US|   30548466|R31SR7REWNX7CF|B00KX4TORI|     170101802|Kingston Digital ...|              PC|          5|            0|          0|   N|                Y|Good quality, wor...|Good quality,work...| 2015-08-31|        1|\n",
      "|         US|     589298| RVBP8I1R0CTZ8|B00P17WEMY|     206124740|White 9 Inch Unlo...|              PC|          3|            1|          2|   N|                Y|in fact this is t...|This demn tablet ...| 2015-08-31|        0|\n",
      "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|        1|\n",
      "|         US|   50728290|R23AICGEDAJQL1|B0098Y77OG|     177098042|                Acer|              PC|          1|            0|          0|   N|                Y|You get what you ...|After exactly 45 ...| 2015-08-31|        0|\n",
      "|         US|   37802374|R2EY3N4K9W19UP|B00IFYEYXC|     602496520|AzureWave Broadco...|              PC|          5|            3|          4|   N|                Y|Great for Windows...|Replaced my Intel...| 2015-08-31|        1|\n",
      "|         US|   52027882| RC9AW4HKJ016M|B0091ITP0S|     977217357|HDE Rotating iPad...|              PC|          1|            0|          0|   N|                Y|            One Star|IT HAS ALREADY CR...| 2015-08-31|        0|\n",
      "|         US|   41770239|R2ALWJE9N6ZBXD|B008I21EA2|     295632907|Linksys AC1750 Wi...|              PC|          1|            0|          0|   N|                N|   Very Disappointed|Very disappointed...| 2015-08-31|        0|\n",
      "|         US|   42560427|R2G5FPA4OX37GV|B00MRB7SBO|     922591915|iPad Pro 9.7, iPa...|              PC|          5|            1|          1|   N|                Y|          Five Stars|Works well. I use...| 2015-08-31|        1|\n",
      "|         US|   46345923|R1IKTSEVXSIMOD|B00LLER2CS|     997551273|SanDisk 16GB CZ43...|              PC|          5|            0|          0|   N|                Y|The encryption so...|The encryption so...| 2015-08-31|        1|\n",
      "|         US|   41751192|R2YA6G6SRFEWF6|B00B0CQCCC|     937999925|TRENDnet Wireless...|              PC|          1|            0|          1|   N|                Y|Didn't last 2 years.|I have owned this...| 2015-08-31|        0|\n",
      "|         US|   21176481| RS9H1N9I3Z1IA|B00GU8W5AE|      13865167|Redragon M901 PER...|              PC|          5|            0|          0|   N|                Y|Awesome gaming mouse|My first gaming m...| 2015-08-31|        1|\n",
      "|         US|   10674058| RKKLBI76VTDNT|B00XHMXJQ0|     967483469|Mudder MHL Adapte...|              PC|          1|            0|          0|   N|                Y|            One Star|I cannot get it t...| 2015-08-31|        0|\n",
      "|         US|   43341796|R2NJ3WFUS4E5G6|B00YGJJQ6U|     986548413|Fintie iPad Air 2...|              PC|          4|            0|          0|   N|                Y|Great choices of ...|Love that Finite ...| 2015-08-31|        1|\n",
      "|         US|   13232866|R21PTQNLGCBN0I|B00XMN20Y6|     873354048|Fintie iPad 2/3/4...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Nice color, I lov...| 2015-08-31|        1|\n",
      "|         US|   29333557|R3G4RT3EQ9RSY7|B00MA40W9I|     535866197|Egoway® New Lapto...|              PC|          1|            0|          0|   N|                Y|Totally wasted $6...|Totally wasted $6...| 2015-08-31|        0|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_reviews = FileManager.open_csv_file(PATH, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e39c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously defined relevant columns for the activity.\n",
    "RELEVANT_COLUMNS_FOR_CHARACTERIZATION = [\n",
    "  \"star_rating\",\n",
    "  \"helpful_votes\",\n",
    "  \"total_votes\",\n",
    "  \"vine\",\n",
    "  \"verified_purchase\",\n",
    "  \"review_date\",\n",
    "  \"sentiment\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7e3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_filtered = df_reviews.select(*RELEVANT_COLUMNS_FOR_CHARACTERIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507d63d",
   "metadata": {},
   "source": [
    "### Generación de las particiones\n",
    "\n",
    "Se implementó un procedimiento automático en PySpark que:\n",
    "\n",
    "1. Filtra los registros de la base de datos que cumplen con cada combinación de valores.\n",
    "2. Almacena cada subconjunto en un diccionario indexado por nombre de combinación (ej. \"R5_VPY_VN\" para `star_rating`=5, `verified_purchase`=Y, `vine`=N).\n",
    "3. Imprime la cantidad de registros por partición para control y trazabilidad.\n",
    "\n",
    "Las particiones con muy pocos registros pueden ser descartadas en etapas posteriores para evitar problemas en el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad608743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitioningManager:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_probabilities(df, cols):\n",
    "        \"\"\"\n",
    "        Computes and returns the probability of each combination of values in the specified columns.\n",
    "        \"\"\"\n",
    "        total_count = df.count()\n",
    "        return df.groupBy(cols).count() \\\n",
    "                 .withColumn(\"probability\", F.round(F.col(\"count\") / total_count, 6)) \\\n",
    "                 .orderBy(\"probability\", ascending=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_partition(df, star_rating, verified_purchase, vine):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame by specific values for rating, verified purchase, and vine.\n",
    "        \"\"\"\n",
    "        return df.filter(\n",
    "            (F.col(\"star_rating\") == star_rating) &\n",
    "            (F.col(\"verified_purchase\") == verified_purchase) &\n",
    "            (F.col(\"vine\") == vine)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_all_partitions(df, min_probability=0.0001):\n",
    "        \"\"\"\n",
    "        Generates partitions only for combinations whose joint probability is above min_probability.\n",
    "        \"\"\"\n",
    "\n",
    "        prob_df = PartitioningManager.compute_probabilities(\n",
    "            df, [\"star_rating\", \"verified_purchase\", \"vine\"]\n",
    "        )\n",
    "\n",
    "        filtered_combinations = prob_df.filter(\n",
    "            F.col(\"probability\") >= min_probability\n",
    "        ).select(\"star_rating\", \"verified_purchase\", \"vine\").collect()\n",
    "\n",
    "        partitions = {}\n",
    "        for row in filtered_combinations:\n",
    "            rating = row[\"star_rating\"]\n",
    "            purchase = row[\"verified_purchase\"]\n",
    "            vine = row[\"vine\"]\n",
    "\n",
    "            key = f\"R{rating}_VP{purchase}_V{vine}\"\n",
    "            filtered = PartitioningManager.filter_partition(df, rating, purchase, vine)\n",
    "            partitions[key] = filtered\n",
    "            print(f\"Partition {key} created with {filtered.count()} records.\")\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def stratified_sample_partitioned_data(partitions_dict, label_col=\"sentiment\", fraction=0.3, min_rows=50):\n",
    "        \"\"\"\n",
    "        Applies stratified sampling to each partition based on sentiment.\n",
    "        \"\"\"\n",
    "        sampled_partitions = {}\n",
    "\n",
    "        for key, df in partitions_dict.items():\n",
    "            count = df.count()\n",
    "\n",
    "            if count < min_rows:\n",
    "                print(f\"Skipping partition {key} — only {count} rows (<{min_rows})\")\n",
    "                continue\n",
    "\n",
    "            sentiments = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            fractions = {s: fraction for s in sentiments}\n",
    "\n",
    "            sampled_df = df.sampleBy(label_col, fractions, seed=42)\n",
    "            sampled_partitions[key] = sampled_df\n",
    "            print(f\"Sampled {sampled_df.count()} rows from partition {key} (original: {count})\")\n",
    "\n",
    "        return sampled_partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def build_combined_sample(partitions_sampled_dict):\n",
    "        \"\"\"\n",
    "        Unites all sampled partitions into a single DataFrame (M).\n",
    "        This helps reduce computational load while maintaining diversity.\n",
    "        \"\"\"\n",
    "        if not partitions_sampled_dict:\n",
    "            raise ValueError(\"No partitions provided for sample combination.\")\n",
    "\n",
    "        combined_df = None\n",
    "        for key, df in partitions_sampled_dict.items():\n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = combined_df.union(df)\n",
    "            print(f\"Partition {key} added to the combined sample.\")\n",
    "\n",
    "        print(f\"Total records in combined sample: {combined_df.count()}\")\n",
    "        return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dbca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN created with 3679909 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPY_VN created with 1019728 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPY_VN created with 603371 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPY_VN created with 443364 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VN created with 410073 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPY_VN created with 300544 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VN created with 152779 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VN created with 135197 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VN created with 65398 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VN created with 59973 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VY created with 15604 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VY created with 13240 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VY created with 4886 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VY created with 1634 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VY created with 705 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VY created with 101 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "partitions = PartitioningManager.generate_all_partitions(df_reviews, min_probability=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec01c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|        1|\n",
      "|         US|   43341796|R2NJ3WFUS4E5G6|B00YGJJQ6U|     986548413|Fintie iPad Air 2...|              PC|          4|            0|          0|   N|                Y|Great choices of ...|Love that Finite ...| 2015-08-31|        1|\n",
      "|         US|   12246861|R2WV71J88ERO3H|B00J082KFQ|      55148922|Case Logic BPCA-1...|              PC|          4|            0|          0|   N|                Y|  Slimmer laptop bag|This is pretty mu...| 2015-08-31|        1|\n",
      "|         US|   23531656|R1DG8L9CSXXPJK|B008IFXQFU|     422480840|TP- Link Wireless...|              PC|          4|            0|          0|   N|                Y|          Four Stars|It works for its ...| 2015-08-31|        1|\n",
      "|         US|   28827675|R1PHDY2F126NRD|B00MB80WE8|     763447327|Prontotec Axius N...|              PC|          4|            0|          0|   N|                Y|So affordable & w...|This was bought f...| 2015-08-31|        1|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Sample of one of the generated partitions.\n",
    "partitions[\"R4_VPY_VN\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ba004",
   "metadata": {},
   "source": [
    "## Técnica de muestreo aplicada por partición\n",
    "\n",
    "Una vez construidas las particiones, se aplicó una técnica de **muestreo estratificado** sobre cada subconjunto, usando la variable `sentiment` como variable de estratificación. Esto asegura que cada muestra mantenga la proporción original de clases de sentimiento en la partición.\n",
    "\n",
    "Para evitar particiones con tamaños insuficientes, se definió un umbral mínimo (`min_rows`) que descarta automáticamente las particiones con muy pocos registros.\n",
    "\n",
    "Además, se permitió configurar el porcentaje de muestreo (`fraction`) por clase de sentimiento. Esto ofrece flexibilidad para ajustar el tamaño del conjunto de entrenamiento o validación según necesidades posteriores.\n",
    "\n",
    "#### Justificación del muestreo estratificado\n",
    "\n",
    "* **Preservación del equilibrio de clases**: Al muestrear por clase de sentimiento se evitan sesgos por clases desbalanceadas.\n",
    "* **Relevancia contextual**: Al aplicar el muestreo dentro de cada partición (y no sobre la base completa), se conserva la variabilidad contextual de las reseñas.\n",
    "* **Evita el submuestreo accidental**: Las particiones muy pequeñas son descartadas de forma controlada, garantizando que el conjunto final tenga representatividad suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb6bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 184082 rows from partition R5_VPY_VN (original: 3679909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 51148 rows from partition R4_VPY_VN (original: 1019728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 30183 rows from partition R1_VPY_VN (original: 603371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 22223 rows from partition R3_VPY_VN (original: 443364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 20507 rows from partition R5_VPN_VN (original: 410073)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 15034 rows from partition R2_VPY_VN (original: 300544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 7595 rows from partition R1_VPN_VN (original: 152779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 6708 rows from partition R4_VPN_VN (original: 135197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3345 rows from partition R3_VPN_VN (original: 65398)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3066 rows from partition R2_VPN_VN (original: 59973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 817 rows from partition R5_VPN_VY (original: 15604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 724 rows from partition R4_VPN_VY (original: 13240)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 266 rows from partition R3_VPN_VY (original: 4886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 93 rows from partition R2_VPN_VY (original: 1634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 35 rows from partition R1_VPN_VY (original: 705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3 rows from partition R5_VPY_VY (original: 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_partitions = PartitioningManager.stratified_sample_partitioned_data(partitions, fraction=0.05, min_rows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e764fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN added to the combined sample.\n",
      "Partition R4_VPY_VN added to the combined sample.\n",
      "Partition R1_VPY_VN added to the combined sample.\n",
      "Partition R3_VPY_VN added to the combined sample.\n",
      "Partition R5_VPN_VN added to the combined sample.\n",
      "Partition R2_VPY_VN added to the combined sample.\n",
      "Partition R1_VPN_VN added to the combined sample.\n",
      "Partition R4_VPN_VN added to the combined sample.\n",
      "Partition R3_VPN_VN added to the combined sample.\n",
      "Partition R2_VPN_VN added to the combined sample.\n",
      "Partition R5_VPN_VY added to the combined sample.\n",
      "Partition R4_VPN_VY added to the combined sample.\n",
      "Partition R3_VPN_VY added to the combined sample.\n",
      "Partition R2_VPN_VY added to the combined sample.\n",
      "Partition R1_VPN_VY added to the combined sample.\n",
      "Partition R5_VPY_VY added to the combined sample.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in combined sample: 345829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sample_M = PartitioningManager.build_combined_sample(sampled_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792c0d5",
   "metadata": {},
   "source": [
    "## Preparación del conjunto de entrenamiento y prueba\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897e09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalAnalysisHelper():\n",
    "    @staticmethod\n",
    "    def dataset_dimensions(df_input):\n",
    "        print(\"columns in the dataset:\", len(df_input.columns))\n",
    "        print(\"rows in the dataset:\", df_input.count())\n",
    "\n",
    "    @staticmethod\n",
    "    def schema_information(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the current schema of the data.\n",
    "        \"\"\"\n",
    "        df_input.printSchema()\n",
    "\n",
    "    @staticmethod\n",
    "    def descriptive_statistics(df_input):\n",
    "        \"\"\"\n",
    "        This method shows the descriptive statistics of the data.\n",
    "        \"\"\"\n",
    "        df_input.summary().show(truncate=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def missing_values_table(df_input):\n",
    "        \"\"\"\n",
    "        Displays a table with the count of missing values per column.\n",
    "        \"\"\"\n",
    "        missing_exprs = []\n",
    "        \n",
    "        for c in df_input.schema.fields:\n",
    "            field_name = c.name\n",
    "            field_type = c.dataType\n",
    "            \n",
    "            if isinstance(field_type, (DoubleType, FloatType)):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | isnan(col(field_name)), field_name)).alias(field_name)\n",
    "                )\n",
    "            elif isinstance(field_type, StringType):\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull() | (col(field_name) == \"\"), field_name)).alias(field_name)\n",
    "                )\n",
    "            else:\n",
    "                missing_exprs.append(\n",
    "                    count(when(col(field_name).isNull(), field_name)).alias(field_name)\n",
    "                )\n",
    "\n",
    "        df_missing_values = df_input.select(missing_exprs)\n",
    "\n",
    "        return df_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b228315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in the dataset: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 210:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows in the dataset: 345829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.dataset_dimensions(df_sample_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8615d944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: integer (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.schema_information(df_sample_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055ddf0",
   "metadata": {},
   "source": [
    "Este esquema describe la estructura de un conjunto de datos de reseñas de productos, ya preparado para análisis en PySpark. Todas las columnas están correctamente tipadas para facilitar tareas de preprocesamiento, modelado y visualización, por lo que no requieren cambios de tipo adicionales. A continuación se describe cada variable:\n",
    "\n",
    "- marketplace (string): Código del país o región del mercado (por ejemplo, \"US\", \"MX\").\n",
    "- customer_id (integer): Identificador único del cliente que realizó la reseña.\n",
    "- review_id (string): Identificador único de la reseña.\n",
    "- product_id (string): Identificador único del producto reseñado.\n",
    "- product_parent (integer): ID grupal de productos relacionados (puede representar variantes).\n",
    "- product_title (string): Nombre o título del producto.\n",
    "- product_category (string): Categoría del producto.\n",
    "- star_rating (integer): Calificación en estrellas otorgada al producto (usualmente de 1 a 5).\n",
    "- helpful_votes (integer): Número de votos que consideraron útil la reseña.\n",
    "- total_votes (integer): Número total de votos recibidos por la reseña.\n",
    "- vine (string): Indica si la reseña es parte del programa Vine (\"Y\" o \"N\").\n",
    "- verified_purchase (string): Indica si la reseña proviene de una compra verificada (\"Y\" o \"N\").\n",
    "- review_headline (string): Título de la reseña.\n",
    "- review_body (string): Cuerpo completo del texto de la reseña.\n",
    "- review_date (date): Fecha en que se publicó la reseña.\n",
    "- sentiment (integer): Etiqueta de sentimiento preprocesada, usada como variable objetivo para modelos supervisados (por ejemplo, 1 = positivo, 0 = negativo).\n",
    "\n",
    "\n",
    "Este esquema permite realizar tanto análisis exploratorio como modelado predictivo con modelos de machine learning supervisados y no supervisados en PySpark sin necesidad de transformación adicional de tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc7f9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 22:32:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 213:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|summary|marketplace|customer_id         |review_id     |product_id         |product_parent      |product_title                                                                                 |product_category|star_rating       |helpful_votes     |total_votes       |vine  |verified_purchase|review_headline                   |review_body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |sentiment          |\n",
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|count  |345829     |345829              |345829        |345829             |345829              |345829                                                                                        |345829          |345829            |345829            |345829            |345829|345829           |345829                            |345829                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |345829             |\n",
      "|mean   |NULL       |2.7991213281448346E7|NULL          |7.765654862257862E9|4.9180527983244896E8|1784.6153846153845                                                                            |NULL            |4.086025174291341 |1.4509280598214724|1.9227595140951164|NULL  |NULL             |696.6666666666666                 |3.75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0.763351251630141  |\n",
      "|stddev |NULL       |1.5591539570539333E7|NULL          |3.572692350202524E9|2.8897488575035936E8|1583.630463887913                                                                             |NULL            |1.3622617427344736|46.442456581791674|49.08379117291993 |NULL  |NULL             |1254.9528543601416                |4.272001872658765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |0.42502545879407655|\n",
      "|min    |US         |10157               |R1000CCCY44YL4|0132793040         |1602                |#1080CC Pelican HardBack Case (with Computer Case Liner)                                      |PC              |1                 |0                 |0                 |N     |N                |\u001a\u001a\u001a\u001a\u001a\u001a                            |\u001d\u001dWhile the texture and feel of this keyboard iPad case feels pretty good, I have to agree with another Vine reviewer that the keyboard is awkward. While the keyboard is about the size of a regular wireless Apple keyboard, the keys seem to be moved a little to the left, so it doesn't feel like normal typing. Even more problematic is when you click the space key, your thumb hits up against the edge of the case. So the basic poor design of the keyboard makes it not comfortable for typing, unfortunately.|0                  |\n",
      "|25%    |NULL       |14401893            |NULL          |6.465354276E9      |242889999           |700.0                                                                                         |NULL            |4                 |0                 |0                 |NULL  |NULL             |5.0                               |1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|50%    |NULL       |26724274            |NULL          |9.875971812E9      |488442804           |700.0                                                                                         |NULL            |5                 |0                 |0                 |NULL  |NULL             |7.0                               |1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|75%    |NULL       |42723957            |NULL          |9.966634975E9      |743709280           |3911.0                                                                                        |NULL            |5                 |1                 |1                 |NULL  |NULL             |1333.0                            |3.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |1                  |\n",
      "|max    |US         |53096561            |RZZZD8HUINFU1 |B01MU3GE5L         |999982788           |☆ 4 Port Powered USB 3.0 Hub Portable for MacBook Air, Windows 8 Tablet PC ☆ Multiple Splitter|PC              |5                 |24714             |26143             |Y     |Y                |😒 Worst purchase I've made online|😍👍👍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |1                  |\n",
      "+-------+-----------+--------------------+--------------+-------------------+--------------------+----------------------------------------------------------------------------------------------+----------------+------------------+------------------+------------------+------+-----------------+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "StatisticalAnalysisHelper.descriptive_statistics(df_sample_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c34cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "|marketplace|customer_id|review_id|product_id|product_parent|product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|review_headline|review_body|review_date|sentiment|\n",
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "|0          |0          |0        |0         |0             |0            |0               |0          |0            |0          |0   |0                |0              |0          |0          |0        |\n",
      "+-----------+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+-----------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_values = StatisticalAnalysisHelper.missing_values_table(df_sample_M)\n",
    "missing_values.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a51a9b",
   "metadata": {},
   "source": [
    "El contenido mostrado indica que el dataset no contiene valores nulos, vacíos ni faltantes en ninguna de sus columnas, lo cual es ideal para su uso en modelos de machine learning. Todas las columnas tienen registros válidos, incluso aquellas de tipo texto, entero o fecha. La fila con valores \"0\" representa probablemente un ejemplo simbólico o una validación inicial de integridad, no un error.\n",
    "\n",
    "Dado que no hay datos vacíos ni inconsistentes, no se requieren procesos adicionales de limpieza, imputación ni eliminación de filas o columnas, lo que permite avanzar directamente al análisis exploratorio, transformación de características o entrenamiento de modelos con confianza en la calidad del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8ce89",
   "metadata": {},
   "source": [
    "## Preparación del conjunto de entrenamiento y prueba\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ac0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestManager:\n",
    "  @staticmethod\n",
    "  def stratified_train_test_split(df, label_col, train_ratio : int, seed : int):\n",
    "    \"\"\"\n",
    "    Performs a stratified split of the DataFrame based on the label column.\n",
    "    Returns (train_df, test_df).\n",
    "    \"\"\"\n",
    "    label_values = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    train_fractions = {label: train_ratio for label in label_values}\n",
    "\n",
    "    train_df = df.sampleBy(label_col, train_fractions, seed=seed)\n",
    "\n",
    "    train_ids = train_df.select(F.monotonically_increasing_id().alias(\"id\"))\n",
    "    df_with_id = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    test_df = df_with_id.join(train_ids, on=\"id\", how=\"left_anti\").drop(\"id\")\n",
    "    train_df = train_df.drop(\"id\") if \"id\" in train_df.columns else train_df\n",
    "\n",
    "    print(f\"Train set: {train_df.count()} rows\")\n",
    "    print(f\"Test set: {test_df.count()} rows\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7de65e",
   "metadata": {},
   "source": [
    "## COnstrucción de conjunto de entrenamiento y prueba\n",
    "---\n",
    "\n",
    "Se realizó una división estratificada del DataFrame df_sample_M utilizando la clase TrainTestManager. Esto significa que los datos fueron separados en entrenamiento y prueba manteniendo la proporción original de las clases presentes en la columna \"sentiment\", que es la variable objetivo.\n",
    "\n",
    "\n",
    "Recopila todos los valores únicos de la clase (label_col = \"sentiment\").\n",
    "Aplica un muestreo estratificado con una fracción de 80% para entrenamiento (train_ratio = 0.8) y el 20% restante para prueba.\n",
    "Usa una semilla fija (seed = 42) para garantizar reproducibilidad.\n",
    "Se asegura de que los datos del conjunto de prueba no se solapen con los del conjunto de entrenamiento.\n",
    "\n",
    "Resultado de la división:\n",
    "\n",
    "Training set: 276,731 muestras (80%)\n",
    "Test set: 69,098 muestras (20%)\n",
    "\n",
    "\n",
    "Este enfoque mejora la generalización del modelo, especialmente cuando se trabaja con clases desbalanceadas, ya que cada clase está representada proporcionalmente en ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b16eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 276731 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 69098 rows\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = TrainTestManager.stratified_train_test_split(df_sample_M, \"sentiment\", 0.8, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f6a82",
   "metadata": {},
   "source": [
    "## Selección de métricas de calidad\n",
    "---\n",
    "Accuracy:\n",
    ">Accuracy mide la proporción de predicciones correctas que hace el modelo sobre el total de casos. Es decir, cuántas veces el modelo acierta, sin distinguir entre clases. Es útil cuando las clases están balanceadas y los errores tienen el mismo peso. Sin embargo, puede ser engañosa si hay desbalance de clases, ya que un modelo puede tener alta exactitud simplemente prediciendo siempre la clase mayoritaria.\n",
    "\n",
    "Precision\n",
    ">La precisión (precision) es una métrica que indica qué proporción de las predicciones positivas realizadas por el modelo fueron correctas. Es especialmente importante en contextos donde los falsos positivos tienen un alto costo, como en sistemas de detección de fraude, diagnósticos médicos o filtrado de spam. Una alta precisión significa que el modelo es confiable al identificar positivos, ya que comete pocos errores al etiquetar como tal. Su importancia radica en que permite evaluar la exactitud del modelo cuando afirma que una instancia pertenece a una clase positiva, ayudando a evitar consecuencias negativas derivadas de decisiones incorrectas.\n",
    "\n",
    "f1_score\n",
    ">F1-score es la media armónica entre la precisión (precision) y la exhaustividad (recall). Es una métrica clave cuando hay un desbalance entre clases o cuando los falsos positivos y falsos negativos tienen consecuencias importantes. Sirve para tener una visión más equilibrada del desempeño del modelo que usando solo accuracy, ya que castiga los extremos donde una métrica es alta y la otra baja.\n",
    "\n",
    "precision\n",
    ">Precision indica qué proporción de las predicciones positivas realmente lo son. Es esencial cuando el costo de un falso positivo es alto. Por ejemplo, si un modelo predice que un producto es defectuoso y no lo es, puede generar gastos innecesarios. Por eso, esta métrica evalúa qué tan \"confiables\" son los aciertos positivos del modelo.\n",
    "\n",
    "recall\n",
    ">Recall (también llamado sensibilidad o exhaustividad) mide la capacidad del modelo para identificar todos los casos positivos reales. Es vital cuando los falsos negativos son especialmente graves, como en el diagnóstico médico o detección de fraudes. Un alto recall asegura que el modelo no está dejando pasar demasiados positivos verdaderos sin detectar.\n",
    "\n",
    "ROC AUC\n",
    ">La ROC AUC (Receiver Operating Characteristic - Area Under the Curve) es una métrica que evalúa la capacidad de un modelo para distinguir entre clases positivas y negativas. La curva ROC compara la tasa de verdaderos positivos (recall) frente a la tasa de falsos positivos a diferentes umbrales de decisión, y el área bajo esta curva (AUC) resume el rendimiento del modelo en un solo valor entre 0 y 1. Un AUC de 1.0 indica un modelo perfecto, mientras que 0.5 sugiere un modelo sin capacidad predictiva (equivalente a adivinar al azar). Su importancia radica en que es independiente del umbral y útil especialmente cuando hay clases desbalanceadas, ya que proporciona una visión global del comportamiento del modelo más allá de una única métrica como precisión o recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb047e",
   "metadata": {},
   "source": [
    "## Construcción de modelo de aprendizaje\n",
    "--- \n",
    "La clase ManualLogisticRegressionManager se encarga de gestionar todo el proceso necesario para entrenar, preprocesar y evaluar un modelo de regresión logística binaria utilizando PySpark. Primero, permite preparar los datos mediante un pipeline que ensambla las variables predictoras y las escala para que tengan una magnitud comparable. Luego, entrena el modelo aplicando validación cruzada con una búsqueda de hiperparámetros (regParam y elasticNetParam) para encontrar la mejor configuración posible, evaluando el rendimiento con el área bajo la curva ROC (AUC). Finalmente, evalúa el modelo generando métricas clave como la precisión, recall, F1-score, accuracy y el propio AUC, lo que permite tener una visión completa del desempeño del modelo en tareas de clasificación binaria. Esta clase automatiza y organiza eficientemente cada una de las etapas del flujo de trabajo del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "573aa82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLogisticRegressionManager:\n",
    "    @staticmethod\n",
    "    def evaluate_binary_classification(predictions, label_col: str = \"label\", prediction_col: str = \"prediction\", probability_col: str = \"probability\"):\n",
    "        \"\"\"\n",
    "        Evalúa un modelo de clasificación binaria en base a varias métricas estándar.\n",
    "        \"\"\"\n",
    "        accuracy = predictions.filter(F.col(label_col) == F.col(prediction_col)).count() / predictions.count()\n",
    "        \n",
    "        tp = predictions.filter((F.col(prediction_col) == 1) & (F.col(label_col) == 1)).count()\n",
    "        fp = predictions.filter((F.col(prediction_col) == 1) & (F.col(label_col) == 0)).count()\n",
    "        fn = predictions.filter((F.col(prediction_col) == 0) & (F.col(label_col) == 1)).count()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0.0\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=probability_col, metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            \"accuracy\": [accuracy],\n",
    "            \"precision\": [precision],\n",
    "            \"recall\": [recall],\n",
    "            \"f1_score\": [f1],\n",
    "            \"auc\": [auc]\n",
    "        })\n",
    "\n",
    "        return results_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_data(df_train, df_test, feature_cols, label_col=\"sentiment\"):\n",
    "        \"\"\"\n",
    "        Arma un pipeline para ensamblar y escalar características.\n",
    "        \"\"\"\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "        scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withStd=True, withMean=False)\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "        pipeline_model = pipeline.fit(df_train)\n",
    "        train_preprocessed = pipeline_model.transform(df_train)\n",
    "        test_preprocessed = pipeline_model.transform(df_test)\n",
    "\n",
    "        return train_preprocessed, test_preprocessed\n",
    "\n",
    "    @staticmethod\n",
    "    def train_logistic_regression(train_df, test_df, label_col=\"sentiment\"):\n",
    "        \"\"\"\n",
    "        Trains a logistic regression model on the training set and evaluates it on the test set.\n",
    "        Returns the trained model and evaluation metrics.\n",
    "        \"\"\"\n",
    "        lr = LogisticRegression(labelCol=label_col, featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(lr.regParam, [0.1])\n",
    "            .addGrid(lr.elasticNetParam, [0.0])\n",
    "            .build())\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "        cv = CrossValidator(estimator=lr,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=2,\n",
    "                            parallelism=2)\n",
    "\n",
    "        cv_model = cv.fit(train_df)\n",
    "        best_model = cv_model.bestModel\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        print(f\"BEst model: RegParam = {best_model._java_obj.getRegParam()}, ElasticNet = {best_model._java_obj.getElasticNetParam()}\")\n",
    "        \n",
    "        return predictions, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8feafb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features = [\"star_rating\", \"helpful_votes\", \"total_votes\"]\n",
    "\n",
    "train_pp, test_pp = ManualLogisticRegressionManager.preprocess_data(\n",
    "  train_df.sample(withReplacement=False, fraction=0.1, seed=42),\n",
    "  test_df.sample(withReplacement=False, fraction=0.1, seed=42),\n",
    "  feature_cols=features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562ce0a",
   "metadata": {},
   "source": [
    "## Análisis de resultados\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26e13d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEst model: RegParam = 0.1, ElasticNet = 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions, best_model = ManualLogisticRegressionManager.train_logistic_regression(train_pp, test_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ee0a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = ManualLogisticRegressionManager.evaluate_binary_classification(predictions, label_col=\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "408b1133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.933079</td>\n",
       "      <td>0.910404</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>0.953014</td>\n",
       "      <td>0.999809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score       auc\n",
       "0  0.933079   0.910404  0.999809  0.953014  0.999809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f077e5c",
   "metadata": {},
   "source": [
    "El modelo de regresión logística binaria obtuvo resultados excepcionales según las métricas evaluadas, lo que indica un rendimiento altamente confiable y preciso. La exactitud (accuracy) alcanzó un 93.31%, lo que significa que el modelo clasificó correctamente más del 93% de los casos en general, una cifra muy alta que refleja su consistencia global.\n",
    "\n",
    "La precisión fue de 91.04%, lo que implica que, de todas las instancias que el modelo predijo como positivas, más del 91% realmente lo eran. Esto es especialmente importante en contextos donde los falsos positivos tienen un alto costo, ya que ayuda a reducir errores en decisiones automatizadas.\n",
    "\n",
    "Por otro lado, el recall fue prácticamente perfecto, con un 99.98%, lo que indica que el modelo logró identificar casi todos los casos positivos reales. Esta métrica es crucial en aplicaciones donde los falsos negativos son críticos, como en la detección de fraudes, diagnósticos médicos o control de calidad.\n",
    "\n",
    "El F1 Score, que combina precisión y recall en una única métrica armónica, alcanzó un 95.30%, lo que muestra un excelente balance entre ambas capacidades. Esto refuerza que el modelo no solo es preciso, sino también exhaustivo al identificar correctamente las clases.\n",
    "\n",
    "Finalmente, el AUC-ROC fue de 0.9998, una métrica que evalúa la capacidad del modelo para distinguir entre las clases. Un valor tan cercano a 1 demuestra que el modelo tiene un poder discriminativo casi perfecto, es decir, que puede separar muy bien los positivos de los negativos en diferentes umbrales de decisión.\n",
    "\n",
    "Tiene un alto desempeño y se valida la solidez del modelo para ser implementado en entornos de producción donde se requiere alta confiabilidad y mínimo margen de error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_4_big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
